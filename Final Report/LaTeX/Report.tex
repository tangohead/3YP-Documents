\documentclass[]{report}
\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{lscape}
\usepackage{listings}

\begin{document}

\title{Giving Up Smoking: Modelling how Social Networks Impact upon the Breaking of Habits}
\author{Matt Smith}
\date{\today}
\maketitle
\begin{abstract}
This project provides a proof-of-concept agent-based model for the problem of how social networking impacts on the behaviour of smoking cessation. By attempting to build a basic model of human smoking behaviours and defining interactions, this model allows for  simulations using, in theory, any number of autonomous entities. Through simulations, it appears that the quantity and social location of humans in networks significantly impacts their effect within the graph and, by extension, the quantity of smokers present. Generally, the model shows promise as a proof-of-concept for further development both in the areas of quality of implementation and production of results.
\end{abstract}

\tableofcontents

% --------------------------------
%
%			INTRODUCTION
%
% --------------------------------
\chapter{Introduction}

\section{Project Overview}
A great deal of current research in the field of Computer Science focus on tackling real-world problems through a combination of separate theoretical areas, using abstract techniques to represent, merge and ultimate gain new information about these problems. Generally speaking, this project takes the fields of smoking cessation and social networking and, by mapping them to an agent-based model, tries to provide information about how social situations and conditions change the behaviour of someone in giving up smoking. On top of this, it aims to assess whether this would be useful as a starting point for commercial level approach to investigating smoking cessation by considering the implementation in terms of scalability, efficiency and more. 

For each of the constituent fields used within this project, there is the potential for an entire project in each. As such, the scope of this work was limited to an attempt to create a basic model of smoking related behaviours and social interactions. By doing this, it could act as a proof of concept for further work in emulating human behaviour or social networking, should the model indicate that it would be useful. At this point, it should be noted that social network is referred to in the context of all social interaction as opposed to just that of online networking. 

Overall, the project is split into three sections, each building upon the previous. To provide an understanding of any existing useful work as well as where the gaps in the existing research lie, the relevant fields were investigated. This was followed up by the development of the model itself, with a focus on extensibility and maintainability, which involved mapping the problem domains of networking and human behaviour onto abstract representations. The final part of the project was to analyse this model by both considering its features and running sample simulations, the latter aiming to provide some new information about the interplay of smoking cessation and social networks.

\section{Project Rationale}
As described above, the project is focused on smoking cessation and in particular, what factors contribute to individuals to giving up. Smoking is a big problem for healthcare in many countries partly due to the expense managing it - in 2005/06, the National Health Service (NHS) spent around \pounds5.2 billion on costs directly related to smoking\cite{NHS-78}. Adding to this, many smokers are cited as wanting to give up though failing in their attempts\cite{NHS-44}. Understanding the social conditions through which smoking cessation is made more successful would be very useful from the point of view of the NHS and the individual.

Furthermore, due to the advent of services such as Facebook, Twitter and other online social networks, the concept of networking has become significantly more popular in recent years, although it is an ever-present factor in life\cite{USN-4}. Whilst studied for a long time in areas such as sociology, computer representations of these are relatively new. On top of this, there is a lot of interest in the impact of these networks on human behaviours; with reference to smoking cessation, an area of particular interest are that of health being affect due to those in close social proximity. Being able to investigate these effects and understand what changes their severity is a key factor in applying them to giving up smoking.

One approach to this is to perform a wide range of surveys, psychological and sociological studies to try to find out these conditions which could then be replicated in real social circles. Not only would this be very expensive, but the information gained in this kind of study depends on a number of circumstantial factors of the participants and as such, it may not be relevant everywhere, causing further expense and time in re-running this work. Using a computer model presents an alternative that, if developed accurately, can be adapted and tuned to simulate different situations. Ultimately, this project aims to provide an adaptable and configurable model of the effects of social networking upon smoking cessation such that new information can be gained about the problem and, with further development into human behaviours and social interaction, extending the model could provide more insight into how different situations can be manipulated to aid in giving up smoking.

% --------------------------------
%
%		END OF INTRODUCTION
%
% --------------------------------

% --------------------------------
%
%			LIT REVIEW
%
% --------------------------------
\chapter{Research \& Literature Review}
\label{sec:litrev}
\section{Overview}
An underlying requirement of the project is to incorporate a number of different Computer Science fields in order to gain new information; understanding the background to each of these is the key to being able to represent the real-world phenomena. Specifically, investigating how social networks and personal health interact provides a good starting point for creating an agent-based model. This section will consider relevant and similar works to both validate assumptions and allow the project to expand upon the existing knowledgebase.

\section{Social Networking}
\label{sec:litrev-socnet}
Whilst social networks have always existed they have become a popular area of research in recent years; in particular, by applying an analytical approach, a formal method of representation has been created. At the lowest level, networks are represented using mathematical graph theory\cite{NetMark-21}. This means that individual entities within the network are represented as nodes, whilst relations between these entities are edges. Importantly, these edges can be either directed or undirected, which can change their meaning depending on context - for example, directed edges could be used to show how one node likes another\cite{USN-14}. By extension, a bidirectional edge represents some mutual relationship between the nodes.

Building on this basic set of terms, a number of structural definitions emerge that begin to directly relate the theoretical to the practical. A key concept is that of triadic closure, which states that, where two nodes share a common connection, they are more likely to be connected at some point in the future \cite{NetMark-44}. This is particularly influential within social networks, when it comes to working out which pairs of people are going to form connections next. Extending this, the idea of strong \& weak ties brings an extra dimension to the edges between nodes. A strong tie may be likened to a friendship, and a weak tie to an acquaintance, for example. Generally, the stronger ties are present in small, connected clusters whereas weaker ties link these clusters together\cite{NetMark-46}. Again relating this to real-world examples, this is similar to groups of close friends being connected to others by acquaintances in other groups. Quantifying a graph is done through a number of measures. One of these is betweenness, which is calculated by calculating a `flow' through the graph - edges of high flow are important since they carry the most traffic and, as such, have a high betweenness value. This can indicate the strength of a tie; a weak tie is likely to have a high rate of flow, for example, since it is between two highly connected groups and is of high importance in joining these clusters\cite{NetMark-66}.  

With these terms in mind, further concepts that more directly relate to social networks and human behaviours can be introduced. A key example of this is homophily, defined as groups of friends which are similar, where this similarity may manifest itself in beliefs, interests, jobs, or other factors\cite{USN-18}. Although the term provides an overview, there are a number of mechanisms that underpin homophily. When fixed characteristics such as ethnicity are considered selection plays a role, which is the idea that people interact and form relationships with those who they share the most in common. In contrast to this, characteristics which are variable, such as interests or behaviours, show how socialisation and social influence affect the person. The former is the process of individuals striving to bring themselves closer to others with similar characteristics, whereas the latter is when existing connections to others cause changes to the behaviours or interests, which is effectively the antithesis of selection\cite{NetMark-81}. It should be noted that selection and social influence have an amount of interaction that can result in it being difficult to determine which aspect of homophily has contributed towards a connection.

Expanding upon influence within a graphical representation of a social network, there are a number of approaches to emulating real-world influence between networked people. Although there are many specialist models that attempt to recreate this, two basic approaches are:
\begin{itemize}
\item Linear Threshold, which is defined as $\sum\limits_{w \text{ neighbour of } v}^{} b_{v,w} \geq \theta_{v} $ where $v$ is a node, $w$ is a neighbour and $b_{v,w}$ is some weight, such as influence, between them\cite{inf-papers}. This is a basic representation of influence that can effectively be summarised as a node taking on behaviours that its neighbouring nodes also exhibit, depending on some predefined boundary. An example of this in a real social network could be that if more than half of someone's friends play football, they will also begin to play football.
\item Independent Cascade, defined as a series of time-steps during which any `active' nodes attempt to activate any 'inactive' neighbours with a certain probability\cite{inf-papers}. Should a node become `active', it then tries to activate its neighbours, and so on. Once nodes have attempted neighbour activation, they cannot reattempt and as such, the process ends when no more activations are available. Relating this to human behaviour, it may be likened to someone trying to convince friends about an idea; they will not carry on attempting to convince if they fail, but if someone does adopt the idea, they themselves may spread it further.
\end{itemize}
Whilst these influence models are basic, they are useful when it comes to adapting them for a social network. For the purposes of this project, they are not directly applicable, as different aspects of smoking and smoking behaviours may interact when it comes to influence spread, but they serve sufficiently as a basis.


A final, useful aspect of social network research is that of generating or classifying the type of a network. The most basic type is a randomised network, such as that generated by the Erd\"{o}s-R\'{e}nyi model\cite{erdos}, where edges between any given pair of nodes have an equal probability of existing. Naturally, this leads to most nodes having similar degree, i.e. the number of edges connected to a node. When compared to real-life networks, this lacks hubs, which are nodes that have a higher degree than the network average\cite{BA-SciAm}. As such, random networks appear to be too far removed from what one might observe in nature and two other methods emerge with potential uses: small-world and scale-free networks.

Small-world networks are based on the concept of the small-world phenomenon, which is where human society exhibits a structure where the number of social connections between two people is, on average, quite low, indicating a high level of connectivity\cite{milgram}. More formally, small-worlds generally have high clustering and a low average path length, so aim to represent the effect observed by Milgram in a mathematical way – the Watts-Strogatz method provides an approach to generate these networks\cite{small-world}. At a high level, given a starting set of nodes where each is connected to its neighbours, the algorithm considers rewiring edges based on a predefined probability. This allows for the formation of more realistic structures such as hubs within the network and could be used to investigate how small social groups are affected by smoking cessation attempts.

On the other hand, scale-free networks rely on the previously mentioned concept of hubs, who have a higher degree than the average node and are observed in a wide variety of situations, from computer networks, to business alliances and Hollywood actors. A key principle in building these networks is that of preferential attachment, where nodes are more likely to connect to popular, rather than unpopular nodes. Once more, this is seen in a number of situations such as web pages having a higher chance of linking to popular web pages than more obscure sites\cite{BA-SciAm}. Using the Barabási–Albert approach to generate this type of network, the basic idea is that of starting with a simple, connected base and adding nodes incrementally, considering each other node as a connection candidate\cite{BAStat}. The chance of each connection being successful is relative to the degree of the current node, where a higher degree increases the connection chance. The prevalence of hubs in this type of network is useful in judging the effect of influential members of a community.


\section{Smoking Cessation \& Health}

To make an accurate attempt at mapping smoking behaviours to a social network simulation, the basic factors that affect how humans engage in smoking must be understood. There are two aspects to this – both an analysis of the current smoking situation as well as how people go about trying to give up smoking. Furthermore, the impact of socialisation on the health of a person is another important factor in producing this kind of model.

By looking at NHS smoking statistics for 2012\cite{NHS-13}, a number of important pieces of information relative to simulations required for this project. In Britain in 2010, 20\% of the adult population were recorded as being actively smoking, with the average number of cigarettes a day being 12.7\cite{NHS-13}. Using the definition of `heavy smoker' as somebody who smokes more than 20 cigarettes a day\cite{NHS-14} (and from this a `light smoker' being somebody who smokes fewer than 20 and one or more cigarettes a day), it can be seen that the average smoker is not a `heavy smoker'. Adding to this, a study in 2009 displayed that around 67\% of smokers wanted to give up, and, of those questioned, people who had attempted to give up smoking in the last five years were more likely to want to repeat this effort\cite{SmokOmni}. When it comes to commencing smoking, those who begin to smoke after quitting state a number of reasons for their relapse, including stress and their friends being smokers\cite{NHS-43}, indicating a strongly social aspect to smoking actions. It is also more likely that those who are giving up are more likely to relapse than a non-smoker is to begin smoking\cite{NHS-43}.

Although often specific examples, factors contributing to both the commencement and cessation of smoking have been monitored. In developing countries such as Malaysia, smoking (and in this case tobacco chewing) is on the rise with around 61\% of men being classed as smokers\cite{malay}. When analysed with respect to how these people began smoking, a variety of factors, such as gender, ethnicity and alcohol consumption, were measured, however only ethnicity was observed to be an influence in cessation. Whilst this is a single case and may be influenced by a number of socioeconomic factors, an important point to extract is that it is not necessarily a question of thresholds defining when a person starts and stops smoking. Instead, different factors seem to have varying strength in each instance.

More generally, research into cessation factors and their effect on relapse chances shows a multitude of factors that appear to contribute to failed attempts such as previous quitting attempts, presence of other smokers and behaviour/mood changes\cite{UCL-cess}. On the other hand, the work indicated a lack of effect by bodyweight/weight concerns and amount of cigarettes smoked. Although this was an internet based survey study, the observed aspects of the smoking behaviours are interesting as many different factors are involved but only some of these actually effect giving up, whist others only affect relapsing.

In terms of the social aspect of health, the Framingham Heart Study investigates long term health concerns of a large social network\cite{framingham}. Further work has been conducted using the data from the previously mentioned study, particularly with respect to the spread of obesity\cite{obPap}. It was found that there are indications of social interaction playing a role in the presence of obesity within a network. This is crucial, as it is an indication of health being affected by those with whom an individual interacts. It should be noted that the type of tie between persons was significant in its effect; for example, geographically close neighbours had little impact whilst a mutual friendship greatly increases the chance of those involved becoming obese.

Closer to the combination of social networks and smoking cessation, research into the concept of quitting in groups was carried out over 30 years within the Framingham study revealed a number of interesting phenomena\cite{droves}. Firstly, by the end of the study smoking prevalence was much lower and for those left, there was a higher chance of smokers being connected to other smokers, as well as on the periphery of the non-smoking networks. Adding to this, cessation predictors that occurred within the network were contact with other people who were quitting, type of relationship (e.g. co-worker compared to spouse) and educational status. This reinforces the concept that factors in quitting smoking come from many aspects of life, specifically relationships with others. Finally, due to these facts, group quitting appears to be a more natural approach since it utilises the peer-pressure and the avoidance of having to move to the edge of a social circle, all whilst reducing the number of smoking ties.

\section{Agent-Based Modelling}
Central to the project is the use of an agent-based modelling (ABM) approach to simulation. Fundamentally, it defines a series of agents with attributes, who have a set of distinct behaviours through which they may interact with one another, the aim being that information not initially provided to the system may emerge. Furthermore, agents should display autonomy - that is, that they should function without input from outside the model - and that they are social, allowing others the ability to influence their behaviour \cite{repast-tut}. With this in mind, it can be seen how the agents can represent humans with their behaviours and interactions being mapped to smoking-related actions. Furthermore, this means that the model can not only have a certain level of autonomy but also represent an abstract form of socialisation.

On the whole, this technique brings about a number of advantages over other modelling techniques - it allows for emergent phenomena, a more natural way of modelling systems and flexibility\cite{ABMMethTech}. The first is of particular importance since other methods, for example mathematical models, may be bound by strict limits which can in turn limit their possible results to those expected. Autonomy and simple interactions of agents means that beyond the starting state, any number of an extremely large set of end-states can be reached. As such, with elements of randomness involved, unexpected situations can arise in ABMs. On top of this, the descriptiveness of an ABM is important. As mentioned above, the agents are defined in terms of basic behaviours and interactions which are easily relatable to real-world actions. It is arguably easier to break complex systems into small sub-behaviours than attempt to model the entire environment for not only behaviours, but for all participants. The flexibility of this approach adds to this since once this description is decided upon and implemented, it is easy to add more agents, modify the behaviours and so on without having to redefine the whole model.

Obviously, this does not come without disadvantages. One of the key issues with all modelling approaches is that a `general model' cannot be constructed so the model is only useful for its original purpose\cite{UCL-ABM}. By extension, this means that the model has to focus on one area of behaviour (which could be very wide itself), thus removing those which are considered external. This can limit the results of the model since many `external' aspects may have some effect on those modelled. In addition to this, mapping some actions to an ABM is difficult, particularly those in humans such as subjectiveness\cite{ABMMethTech}. When understanding the results of simulations, this kind of omission from the model must be considered since these behaviours can have a major impact on the course of events; for example, irrational choices by one agent might cause a `butterfly effect' over the course of the rest of the run which would change the results dramatically.

Although it is a relatively new approach to modelling, there are a number of examples which demonstrate that it is widely applicable. In a similar capacity to this project, work has been done to model how viruses spread through humans\cite{ABM-IEEE}. Specifically, the inclusion of real-life data allows the simulation to be built and set up using a realistic base, with the aim of understanding how governmental decisions affected the H1N1 epidemic in Mexico. A particular finding of this study was that a lot of agent-based models use survey data as a basis, resulting in a lack of representation of the way in which humans move over time; this is because of the difficulty in tracking and gaining information from specific people. To avoid this, data sources that allows the tracing of individuals, such as phone records, were used to build in this travelling behaviours.

Furthermore, ABM has also been used to investigate how emergency response can work optimally; from terrorist attacks to floods, the technique can be used to understand how both current emergency processes can be improved and new response actions can be added\cite{emergency}. These two methods, optimising existing behaviours and adding new actions, can apply to ABMs which aim to provide understanding into effecting human behaviour. It is noted that for systems which propose such changes where human life is at stake, an amount of verification and validation of the model must be carried out. This emphasises the fact that, for the data to be relevant to real-world situations, the simulation must display an acceptable degree of similarity to said real-world situations.

\section{Similar Work}

To conclude this section, it is worth considering other similar pieces of work, as these can be useful in informing the direction of the project. It does appear that the particular combination of areas that this project is using has not been widely explored, but there are a number of examples that display some common features.

The first is an analysis of how epidemiology, the study of how diseases spread, can be analysed by using a social network and agent-based modelling approach\cite{epid}. By combining these approaches, the researchers found that it allowed a much more complete view of the situation than by studying individual effects alone. This is due to the agent-based approach that provides the opportunity to define interactions and behaviours, many of which can be handled at once. Furthermore, the inherent ability of ABMs to model social interaction means that the social aspects of epidemiology can be investigated more thoroughly. Although they were found to be useful, it was also the case that the researchers emphasised the need for validation, as mentioned above, and that the scope of the model needs to be restricted to avoid the high level of complexity associated with modelling humans.

Closer to the aims of this project, there are a number of pieces of research in regards to the relationship between social networks and smoking\cite{SmokOmni}. Specifically, a three-part approach is taken; the first focuses on modelling addiction and cessation as functions, the second considers influence, and the third deals with generating realistic networks. An extremely detailed solution is proposed in general, where a probability based approach was used within the model to represent different aspects of human behaviours and character\cite{SmokOmni-pap1}. This means that mathematical functions can be defined to produce these probabilistic representations. In terms of influence research, it was found that targeting audiences and indirect influence applied to individuals can be very effective when attempting to change behaviours whilst trying to `force the issue', paint the behaviour as bad or being overly explicit in the message caused a lack of receptiveness\cite{SmokOmni-pap2}. In general, the model uses a combination of peer pressure, implemented in various ways, and health concern when it comes to influencing the decision of whether to cease smoking\cite{SmokOmni-pap3}.

\section{Summary}
In general, whilst there are few similar projects, if the fields are separated into social networks, agent based modelling and smoking cessation, a solid foundation can be constructed, upon which this research may be built. Although the approach will be detailed in full in the next section, this research indicates that a graph based representation of a social network, using humans as nodes within an agent based simulation, offers a common and fruitful way to address this problem.

% --------------------------------
%
%		END OF LIT REVIEW
%
% --------------------------------

% --------------------------------
%
%			IMPLEMENTATION
%
% --------------------------------
\chapter{Model Development \& Implementation}
\section{Overview}
Based on the research detailed above, it was clear to see that the agent-based modelling approach was a good match to the problem domain – at a high level, the social network could be represented by a graph, the humans being agents (and as such nodes in said graph), with connections between them mapping relationships and being edges within the graph. The model, including details as to how the final version was reached, is below.

It was decided during the conception of the project that it should stand up for comparison against a commercial approach. Due to this, a number of design plans were laid in relation to the structure. In general, the solution was written using a `platform' approach so that, as well as being easier to maintain, it was extensible. To accommodate this, good programming practice was followed by ensuring functional separation and suitable abstraction of methods meaning sections such as decision tree branches or interaction rules could be changed or removed without much effort.

Adding to this, a number of input and output methods are supported. Whilst the system incorporates a limited number of ways of generating graphs as social network bases, it also supports sampling other graphs and importing pre-defined ones. Whilst this will be explained in more detail below, it does allow for a great deal of flexibility when it comes to experimenting with how the ABM reacts to non-standard environments. In regards to outputs, two main formats were used. The system can, at any given step in the simulation, feed a list of all agents and their attribute values at that particular moment as well as the social network graph for that step. This is important for analysing how the network changes over time intervals. Furthermore, if necessary, the system can reveal console output to detail what is happening at a given moment though this is more a debugging feature.

Throughout the project, a series of existing technologies and toolkits were utilised; given that graph and simulation techniques were heavily incorporated into the model, using libraries for these was key since they are both complex fields within themselves. Furthermore, by using these tools, more time could be spent on the development of the solution itself rather than the back-end representation and management of data.

Due to the project being research driven, a lot of time was spent referencing existing information so more traditional development methodologies were not directly applicable - in both the initial research section and the development of the model, a loose spiral approach was taken, meaning that there were a number of short cycles, each analysing the work so far, understanding what needs to change then working on those changes [ref spiral]. In particular, this was very useful during the creation of the final model since upon each group of changes, the effect on the behaviour of the model needed to be checked to ensure that it didn't become imbalanced. Although the balancing will be detailed below, issues can stem from the fact that the simulation involves autonomous interaction of many agents, meaning a small change can result in wide-scale effects within the model, often unexpected. Controlling these changes is important as developing on top of an unbalanced model results in subsequent issues later in the project.

\emph{MORE ON PROJECT MANAGEMENT HERE? problems, how to get over them etc}

\section{Technologies \& Tools}
As described previously, existing tools and libraries were critical to the progress of the project. Using these not only sped up development due to many of the fundamental tasks being settled, such as representing graphs, but added to the reliability of the project due to being developed by communities of experts. Due to this, the project could focus specifically on the project at hand and the tools required to achieve this rather than building from the basics.

\subsection{Repast Simphony}
Repast (Recursive Porus Agent Simulation Toolkit) is an agent-based modelling framework which provides a number of tools to simplify the process of developing agents and the environment in which they react\cite{repast-main}. Scheduling tools, a simulation engine and more are provided whilst Repast Simphony provides extra features to further aid in the fast development of ABMs by `wrapping' the features of Repast, allowing for more detailed representation of networks, a more interactive development environment and interaction with tools such as Weka or JUNG.

By utilising this framework, a great deal of time was saved in the project - a wide range of tools such as a scheduler and a network representation would need to be developed otherwise, meaning a large section of the work would be dedicated to writing and testing this. Instead, Repast handles all this through its APIs which not only deal with the previously mentioned tasks but provide many other helper methods that aid in simulation analysis. An example of this is the inclusion of Repast Networks which are modified JUNG graphs specifically for agent-based simulation by extending the JUNG implementation to work with agent-type objects and provide methods of interfacing this to the rest of the simulation environment.

Whilst it is vast and well-constructed framework, using it is not without a steep learning curve since the provided documentation is minimal. As a result, an amount of experimentation is required to use some of the more specialist features, for example incorporating user parameters into the GUI. Largely, this is due to the fact that the user base is naturally limited to those carrying out agent-based simulations and as such the support community is limited to this group. This does have advantages in that those who do offer support tend to have experience in the field. Even with this being the case, the environment is developed and maintained in such a way that through the experimentation, undocumented methods can be understood.

\subsection{JUNG}
JUNG is the Java Universal Graph Network/Graph framework which provides a series of data structures and related methods for storing and manipulating graphs and networks. At the lowest level, it provides a series of abstract definitions meaning that custom classes for nodes and edges can be used to construct said graphs. In particular, a large number of graph metrics such as betweenness centrality can be calculated via the library.

Whilst the sole purpose of this tool is to represent graphs, it was not used to handle the main social network representation due to the fact that using a JUNG graph in the Repast Simphony environment would require a conversion on every step. This is a computationally expensive task, especially once the graph exceeds around 100 nodes so would heavily impact the performance of the ABM itself. Instead of this, it was used to monitor the social network at intervals, using the inbuilt metric algorithms to report on any undesirable behaviours such as excessive clustering.

\subsection{Java}
Java was the main language of the project since a large number of graph toolkits are available such as Repast Simphony and JUNG. On top of this, it provides a large set of standard libraries providing data structures and algorithms useful in maintaining order within the model. Furthermore, due to the established nature of these libraries, they are considerably faster and more reliable than implementations specifically for this project would be. As a side-effect of using Java, the object oriented nature of it lends itself strongly to modular design, reinforcing the platform-based approach described previously and allowing properly designed classes and methods to be swapped in and out as required.

Whilst Java does have a number of positives, the nature of the language brings some disadvantages. Repast Simphony, designed for single workstations or small clusters\cite{repast-simp}, is implemented in and works with Java, but should the model require scaling to larger simulations, it would require reimplementation in C++ to be used with Repast HPC (High Performance Computing)\cite{repast-HPC}. Although this is not ideal, the usage of Java simplifies the model creation since a HPC version requires much greater involvement in scheduling and other low-level operations meaning that this project can serve as a proof of concept.

\subsection{Gephi}
Gephi, a graph visualisation tool, was very useful in understanding how the networks within simulations were changing over time through the use of a number of visualisation methods. It provides the ability to view a number of metrics such as average path length, formatting based on node attributes or viewing graphs using differing layout methods. In particular, viewing graphs using layout algorithms was very useful since it allows an easy, high-level comparison between two graphs, giving an understanding about clustering, hub nodes and size.

\subsection{Git/GitHub}
At a more managerial level, Git, and more specifically GitHub was used for version control and source-code management. The project was maintained in one repository meaning that commits formed different versions. This was very useful during the development of the decision tree and network reconfiguration sections of the agent as it required comparison between versions and monitoring the changes made as it progressed. In addition, using this tool meant that development across a number of separate computers was made much easier and in the case of a series of changes not working as planned, allowed for the rolling back of the code to a given point.

\section{Model Description}
Overall, the model can be split into two main functions – the agent, representing humans within the network, and the network, representing the social connections between agents. Whilst this is the core, there are a number of ancillary functions such as the platform tools and monitoring agents which provide a wide array of features to make the simulation balanced and simple to use.

\subsection{Platform}
\label{sec:platform}
The platform consists of a suite of tools that aid in both the operating and interacting with the model. Broadly, these can be separated into \emph{Input/Output}, \emph{Graph Tools}, \emph{Simulation Monitoring} and \emph{Statistics \& Constants}.

\subsubsection{Input \& Output}
In order to facilitate analysis of the model and the results it produces, the system can input and output in a variety of manners. For input, the aim was to allow users to have a choice in the way that they set the model up whilst output provides feedback as to the simulation state at a given moment. An example of an input requirement is the ability to use a real-world friendship graph which can then be simulated upon, whereas for output, it is useful to be able to graph the change in attributes of nodes over a number of simulation intervals.

To handle the importing and exporting of graphs, support for GraphML is included. GraphML is a way of describing graphs using XML and allows for many types of graphs and importantly, attributes relating to edges and nodes\cite{graphml}. Given the specific needs of this project, existing GraphML generators were not suitable so the Java XML library JAXB was utilised to create a tool that, given a Repast network, would output the XML representing the graph and attributes of constituents of said graph then output it to a GraphML file. In addition to this, the tool can import GraphML as either a complete network (i.e. all attributes stated for nodes and edges) or as a barebones graph. For the former, this is simply converted to a network that can be simulated upon, whereas the latter generates the missing attributes in the same way that creating agents on a system-generated graph would use. This means that as long as a valid graph is provided, it can be used in a simulation.

Another format supported for the importing of graphs into the system is a CSV importer that specifically handles datasets from the Social Network Analysis Project at Stanford University\cite{SNAP}. This is of the form of a list of edges between node IDs, from which a barebones graph can be constructed and filled with attributes in the same manner as a GraphML import. In regards to exporting CSV files, a `snapshot' tool was created that, at a given simulation step or interval, can output all attribute values for all agents in the simulation. As mentioned earlier, this is very useful when it comes to graphing how attributes change over the course of a period of time as well as debugging balancing issues such as feedback loops.

\subsubsection{Graph Tools}
A number of services to handle graph operations have been built to maximise the compatibility and usability of the model – whilst the rationale behind the generation and sampling methods chosen will be detailed in the Social Network section of this document, the tools will be discussed here at a high level.

For generation of graphs, two methods are provided: scale-free and small-world. With each of these methods, the number of nodes required is provided, alongside a number of parameters specific to the generation method. For scale-free graphs, the Barabási–Albert (BA) model is used\cite{BAStat}. It, given a connected graph with more than one node in it (which is generated here by creating a user-specified number of nodes then randomly adding edges until it is connected), sequentially adds nodes, each one being evaluated against all the other nodes in the graph with a chance for each to be connected by an edge. This probability is calculated using the equation in \ref{eq:SF-formula} for a node $i$ where $k_{i}$ is the degree, meaning that edges with a higher than average degree attract more new edges. Once generation is complete, any disconnected nodes are removed since they do not add to the simulation which means that for this method, the user provided number of nodes required is an upper bound, not an exact figure. To aid in the investigation of how existing social features within graphs affect the outcome of simulations, the generator has a feature that allows a base graph to have the BA model applied to it. It should be noted that due to the fact that the base graph does not have connectedness enforced, the output may not be truly scale-free. 
\begin{equation}
\label{eq:SF-formula}
p(k_{i}) = \frac{k_{i}}{\sum\limits_{j}k_{j}}
\end{equation}
Small-world graphs are generated using the Watts-Strogatz model\cite{BAStat}. This method constructs a network of a given size with a user-set average degree by connecting the nodes then circulating around them all, evaluating if they need to be rewired. The full algorithm is detailed in section \ref{sec:GGS} of this report. Usually, this method constructs graphs of high edge density meaning that they have specific use when it comes to analysing them as a network; again, this will be described in detail later.

Whilst the sampling of other graphs was not a focus of the tool development, the ability to provide a JUNG graph and take a sample of this is provided. The method chosen was snowball sampling which is a form of a breadth-first sampler, picking a random node and adding layer by layer of nodes until the required number of nodes are available\cite{snowball}. Although useful, the nature of the sampler means that graphs produced centre around one node which in turn, may represent a very small part of the entire network. Many other sampling algorithms that give better representations of the network-at-large and are easy to include within the system.


\subsubsection{Simulation Monitoring}
During development it was noticed that in some situations, the network suffered from excessive clustering causing most nodes to collapse into an incredibly densely connected group. This prompted the creation of WatchMan, which, although operates as an agent within the simulation, does not interact with the network or any of the other agents. At a user-set interval, the agent can export the current network to a GraphML file, output all attributes of all agents to a CSV file and calculate the network metrics. These metrics are the average clustering coefficient for the entire network, percentages of smokers and those giving up and local clustering coefficients on a series of random points. The aim of the metrics is to provide some insight as to the stability of the network so that if necessary, external intervention can be made.

Although the percentage split of smokers and non-smokers is for manual analysis, the clustering coefficients provide insight for automated adjustments to the network. The clustering coefficient for a node is the probability that two randomly chosen neighbours are themselves connected by an edge\cite{NetMark-44-cluster}. This value being high indicates that the neighbourhood is highly connected; should it be very high, the network is likely to collapse into a small-world type construction. By having WatchMan monitor this and remove edges from nodes with very high clustering coefficients when the network becomes too highly connected, the collapse can be avoided. In regards to the other main feature, it aids in both development and simulation runs since the agent can, as stated, export the current network state to CSV and GraphML formats. By doing this at defined intervals, the progression of the simulation can be monitored after it has finished running, allowing the tracking of specific agent groups, network structures and attribute change. 

\subsubsection{Statistics \& Constants}
To aid in making the model easy to adapt, constants from many aspects of the system are combined into one location for easier editing. This means that simulation parameters such as the means and standard deviations of randomly generated attributes and the maximum number of cigarettes that can be smoked by an agent is all done from a central location. In a similar vein, a number of statistics tools are included in the system. At the most basic level, normally distributed numbers with a given mean and standard deviation can be generated for use anywhere within the model, especially when creating agents.

As stated in the description of WatchMan, statistics based on the network are calculated – using the clustering coefficient algorithm that is part of JUNG, both averages for the whole graph and a number of randomly selected points are used. The former is useful for highly connected graphs such as small-world model ones, whereas the latter is useful in graphs of a scale-free structure, where there may be areas of very high clustering surrounded by less dense areas. To aid in determining what nodes are causing these cases of clustering, a list of high clustering coefficient nodes is returned so that should any graph modifications be required, the ideal nodes are available.

\subsection{Social Network}
As discussed in the Literature Review section (\ref{sec:litrev}), the social network representation within this model is key to the results being relevant and useful. On top of this, the development process revealed a number of aspects of social network modelling that were not initially expected and had to either be worked around or have the model adjusted to work with them. The following section details both the development process and the final solution to the representation of a social network.

\subsubsection{Representation}

At the most basic level, the way in which the network is represented provides the foundation for the model. As found in the research section (\ref{sec:litrev-socnet} of the project), a graph provides the functionality needed to hold both humans and the connections between them at a sufficient level of abstraction. This level of abstraction is important since modelling relationships accurately and with a lot of detail is very difficult due to a wide variety of internal and external factors to said relationship being able to change it, either gradually or suddenly. As is common within ABMs, nodes within the graph represent humans whilst edges map to some relationship between two humans, an example of which can be seen in figure \ref{img:ex-graph}.

\begin{figure}
\label{img:ex-graph}
\begin{center}
\includegraphics[scale=0.25]{example-graph.png}
\end{center}
\caption{Example Network Representation}
\end{figure}

It was decided that the graph should be directed to better model a relationship – undirected edges only indicate a connection, whereas directed edges reveal much more. Working on the principle that a friendship is not necessarily mutual since one person may consider another a friend whilst the other may not reciprocate. Due to this, an edge from node A to node B represents `has a relationship with' and by extension `influences' as seen in figure \ref{img:ex-edge}.

\begin{figure}
\label{img:ex-edge}
\begin{center}
\includegraphics[scale=0.5]{example-edge.png}
\end{center}
\caption{Example Edge: Node A influences Node B}
\end{figure}

\subsubsection{Influence}
The concept of influence is modelled in a manner similar to a probability; it is a weight on an edge with a value between 0 and 1, where 0 represents no influence and 1 represents absolute influence. In general, this makes it easier to calculate multi-hop influence as well and influenced attributes and in doing so, drastically reduces the complexity of operations carried out on a regular basis. Furthermore, a fundamental aim of this model is to keep the complexity to a minimum. Rather than emulate relationships and their influences at a more realistic level, this basic approach was judged more reasonable since it would not require further complexity when it involves it interacting with other elements of the model. 

Importantly, this implementation of influence remaps the concept of positive and negative opinions of a person to a linear scale, for example a person may dislike another, causing them to not want to take on their traits. As such, a negative figure should be viewed as one which has a low influence value and a positive one being seen as holding a higher influence.
Multi-hop influence within the model aims to map to the idea of `friends-of-friends', with the belief being that should an individual's friend be influenced by a third-party, that party should, in turn, influence the individual in some way. In the graph, this is represented by a series of edges from one node to another, as seen in figure \ref{img:multihop-inf}. The extent to which this influence acts on the individual is a matter for tuning of the model and will be discussed in the Agent section of this document. Obviously, the influence at one hop is simply the weight of the influencing edge, whereas to calculate the influence at n hops, the formula seen in figure \ref{eq:multihop-inf} is used, where $w_{i}$ is the influence value of an edge, $w_{0}$ is the edge to the current node and the route from $0$ to $n$ (seen in figure \ref{img:multihop-inf-calc}. By multiplying the influence of each hop together, a likeness to weakening influence across the mutual friends is incorporated along with the method also giving a probability-alike value that can be used when calculating influenced attributes [ref?]. A common situation may be that there are a number of routes to a given destination node from a source and as such there may be a number of possible influence values for this 'friend-of-a-friend'. Once again, simplicity is maintained by taking the highest value – this is based on the assumption that the higher value represents a chain of stronger influences (i.e. friendships) and due to this, a person is likely to opt for that input rather than the ones of weaker influence [ref].

\begin{figure}
\label{img:multihop-inf}
\begin{center}
\includegraphics[scale=0.75]{multi-inf.png}
\end{center}
\caption{Indirect Influence: Influence Across Multiple Hops}
\end{figure}

\begin{equation}
\label{eq:multihop-inf}
\prod_{i=0}^{n}w_{i}
\end{equation}

\begin{figure}
\label{img:multihop-inf-calc}
\begin{center}
\includegraphics[scale=0.75]{multihop-inf.png}
\end{center}
\caption{Calculating Influence Across Multiple Hops}
\end{figure}
Influenced attributes, in this model, are defined as how an individual perceives another's actions affecting themselves, with the intention being that the influence provides some form of weighting against the actual value of the attribute. As will be described later in section \ref{sec:neighbourhood}, part of the \emph{Agent} definition, the idea is for an agent to have an `ideal figure' to which they move towards, where the attributes for this figure are calculated using averages of these influenced attributes. In regards to calculating the attributes, the general formula used can be seen in figure \ref{eq:inf-attr} where $N$ is the neighbourhood of the current node, $\text{attribute}_{n}$ is an attribute value and $\text{influence}_{n}$ is the influence of that node. There were a number of changes that were required for some data types but, since these depend on the specific attribute they will be described in the Agent section (\ref{sec:agent}).

\begin{equation}
\label{eq:inf-attr}
\frac{\sum_{\forall n \in N} \text{attribute}_{n} \times \text{influence}_{n}}{\sum_{\forall n \in N}\text{influence}_{n}}
\end{equation}
From both multi-hop influence and influenced attributes, it can be seen that there is not a direct representation of negative opinion causing a person to act in an opposite manner to the figure they dislike. This is a move away from a realistic behaviour as generally, it would be expected for someone to avoid behaving like a person that they do not get along with, though including this would require some form of state in the relationships between nodes to be maintained. Since it was decided that weighted edges were a simple but expressive method, storing state on these is difficult and requires a great deal of extra computational complexity on each turn to determine the current state as well as how this effects the influence.

\subsubsection{Graph Generation \& Sampling}
\label{sec:GGS}
As described in section \ref{sec:platform}, a number of graph generation and sampling methods are available to provide networks for use within a simulation. Since this system is designed to simulate upon smaller networks than a commercial environment might, using large, real-world networks is not ideal. This is because networks of many thousands of nodes, each requiring many operations to work out influence, attributes and decisions, causes simulations to last for a very long time. As such, an alternative was required that could provide a number of realistic but smaller networks for the simulations.

Scale-free networks were included due to their regularity of appearance within social circles – typically, they provide a low to medium edge density across nodes, with a number of hub nodes\cite{BASciAm}. This is particularly useful since it allows the investigation of `social hubs' within a network to see whether this kind of figure has a specific effect on the smoking behaviours of those around them. A disadvantage with this type of network is that, on the examples generated for this project, nodes tended to have a fairly low degree. Since edges represent relationships here, it could be argued that it is not representative of a realistic group. To counter this, nodes within this graph could be considered as groups of humans (and in effect a small-world network) or simply a less dense friendship group, such as an office of workers where ties are more likely to show acquaintance than friendship. As an aside, this type of graph was most useful for testing the model on since it clearly exhibited any signs of imbalance or lack of change over simulation steps. An example scale-free graph can be seen in figure \ref{img:ex-scale-free} with the generation algorithm in figure \ref{code:sf-code}.

\begin{figure}
\label{img:ex-scale-free}
\begin{center}
\includegraphics[scale=0.25]{ex-scale-free.png}
\end{center}
\caption{Example Scale-Free Graph with 196 Nodes}
\end{figure}

\begin{figure}
\label{code:sf-code}
\begin{enumerate}
\item Begin with a graph of $m_{0}$ nodes, where $m_{0} \geq 2$ and each node has degree of at least 1.
\item Add new nodes incrementally. For each new node consider connecting to each other node $i$ with the probability $p$, calculated using equation \ref{eq:SF-formula} where $k_{i}$ is the degree of $i$.
\caption{Generation Algorithm for Scale-Free Graphs by the Barab\'{a}si-Albert Model\cite{BAStat}}
\end{enumerate}
\end{figure}

Small-world networks are another useful inclusion – the main aim with this type of network was to be able to simulate the effects of groups of close friends like one might find in a club or society\cite{small-world}. In general, a small-world network displays high connectivity relative to the number of nodes present (assuming that the mean number of edges it is generated with is of reasonable size). The key downside of this method is that due to the raised level of connectivity, the chance of small feedback loops forming is very high. This can cause large-scale imbalance within the simulation due to nodes influencing each other to more extreme behaviour and thus exuding this influence to their respective neighbours. As such, this type of network should be used with caution and balanced carefully to avoid these situations – when small-world networks are used, the relevant balancing will be stated. Figure \ref{img:ex-small-world} shows an example small-world network with figure \ref{code:sw-code} displaying the generation algorithm.

\begin{figure}
\label{img:ex-small-world}
\begin{center}
\includegraphics[scale=0.25]{ex-small-world.png}
\end{center}
\caption{Example Small-World Graph with 250 Nodes}
\end{figure}

\begin{figure}
\label{code:sw-code}
For building a network of $N$ nodes, mean degree $K$ and a value $\beta$, where $0 \leq \beta \geq 1$ :
\begin{enumerate}
\item Create a ring lattice of $N$ nodes where each node is connected to its first $K$ neighbours, where it has $\frac{K}{2}$ on each side. For a sparse but connected network, $N \gg K \gg \ln{N} \gg 1$.
\item For each node in the graph, consider each of its edges that connect to yet-unencountered nodes and rewire that edge to connect to a randomly chosen node with probability $\beta$, not allowing self-connection or duplicate edges.
\end{enumerate}
\caption{Generation Algorithm for Small-World Graphs by the Watts-Strogatz Model\cite{BAStat}}
\end{figure}

Snowball sampling was the chosen method due to its simplicity in terms of implementation. Due to the nature of this sampling method, one of a breadth-first search from a random node, it characteristically produces graphs that have one major hub from which most edges emerge. Due to the fact that it can be run on graphs sampled from real-world social networks, the product is often a mix between what might be expected of small-world and scale-free networks. Whilst this was not particularly useful for developing the model upon due to the often high level of connectedness, it does allow for a much wider range of tests to be carried out using the system. This tool is included primarily for the sampling of real-world networks since sampling generated networks is simply replaced with generating networks of the required size. Instead, the number of nodes required can be extracted from datasets sampled from social networks, giving the simulation the ability to run on real graphs. An example of a snowball sample can be seen in figure \ref{img:ex-snowball}, gathered from an email dataset by the SNAP project\cite{SNAP-email}

\begin{figure}
\label{img:ex-snowball}
\begin{center}
\includegraphics[scale=0.25]{ex-snowball.png}
\end{center}
\caption{Example Snowball Sample Graph with 2790 Nodes}
\end{figure}

\subsubsection{Graph Stability}
In the early stages of development a regular occurrence was for the graph to, due to actions undertaken by agents, exhibit very high clustering coefficients for a number of nodes. Due to the complexity of many agents interacting on a regular basis, it was too difficult to balance this entirely through careful parameter selection and agent actions. In order to maintain this a balance, the WatchMan was used to intervene should the graph become too clustered.

The way in which intervention was judged necessary had an effect on the workings of the network and as such, two different methods were implemented and tested. The first of these takes the average clustering coefficient of all nodes in the graph and, if above a system parameter threshold, the 10\% of the nodes in the graph, ordered by highest clustering coefficient first, has each edge (both in and out) considered for removal with a 50\% chance. Whilst an artificial way to maintain balance, it did provide a more stable graph since by thinning out edges on the nodes central to the clusters, feedback loops are removed which helps to slow the compression of the network.

The other method was to choose 10\% of the nodes from the graph at random and calculate the average clustering coefficient at one hop for each. If over a separate system parameter threshold to the above, they are stored as a locally high clustering coefficient. Each of these nodes is then considered in a similar manner to the above method, in that they have their edges considered for removal with the chance of this happening at 50\%. This is a more targeted approach that aims to prevent strongly bound clusters forming at an earlier stage by again thinning out their ties. In testing, the value of the threshold was revealed to be very important as due to the 'early stage' prevention of heavy clustering, the removal had the ability to become overzealous and make the graph sparse. By increasing the threshold for a node to be classed as a locally high clustering coefficient, this method is reserved for only extreme cases.

To illustrate the effects of these stability measures, figure \ref{img:pre-watchman} displays how a non-stabilised graph may appear after 120 steps compared to the stabilised version, figure \ref{img:post-watchman} at around 120 steps. Even though the external intervention does mean that the model loses some faithfulness to real-world interaction in respect to balance, the fact that the network can provide much more useful results by running for longer simulations outweighs this. Furthermore, to have the model balance itself through the interactions themselves would require a significantly more involved representation of agent networking which is beyond the scope of this project.

\begin{figure}
\label{img:pre-watchman}
\begin{center}
\includegraphics[scale=0.25]{pre-balance.png}
\end{center}
\caption{Unstable Graph with 155 Nodes}
\end{figure}

\begin{figure}
\label{img:post-watchman}
\begin{center}
\includegraphics[scale=0.25]{post-balance.png}
\end{center}
\caption{Stable Graph with 152 Nodes}
\end{figure}

\subsection{Agent}
\label{sec:agent}
The agent is the most significant section of the project development, in both operation and time taken to build. Within this section of the ABM, decisions are made about how to change behaviours, influence is acted upon and connections to other nodes are reconfigured; since the social network provides the framework within which the agents act, a lot of the ability to tune the model arises through various aspects of the agent. It is split into two sections: attributes, the data that defines the agent and by extension, the human, and actions which are the functions that an agent can perform in a simulation step.

The basic principle for the agent is that within their local neighbourhood, i.e. the nodes that surround them, influence is used to generate an 'ideal person'. This figure is a set of attributes that is effectively the average of this neighbourhood and to which the agent in question would consider the ideal. Using both these attributes and a number of metrics about the surroundings, the agent then follows a decision tree which provides the chance for an attribute change. On the back of this, the agent can then reconfigure their social connections based on their current attributes, scoring themselves against others in their neighbourhood.

Throughout the development of the model, testing was carried out by running the simulation on a scale-free graph and observing how it behaved. In general, the expected behaviour was for the network to avoid collapsing into a feedback loop and a high average clustering coefficient whilst there being some change within the graph. This was considered to be a display of a balanced model, which is an aim of the project – an example of both a balanced and imbalanced model can be seen in figures \ref{img:ex-simbase}, \ref{img:ex-noncrunch} and \ref{img:ex-crunch}, the difference between the latter two being that the imbalanced network has an extremely high concentration of edges in one sub-area whereas the balance network has a more distributed concentration.

\begin{figure}
\label{img:ex-simbase}
\begin{center}
\includegraphics[scale=0.25]{simbase.png}
\end{center}
\caption{Baseline Graph with 196 Nodes}
\end{figure}
\begin{figure}
\label{img:ex-noncrunch}
\begin{center}
\includegraphics[scale=0.25]{non-crunch.png}
\end{center}
\caption{Balanced Graph with 196 Nodes}
\end{figure}
\begin{figure}
\label{img:ex-crunch}
\begin{center}
\includegraphics[scale=0.25]{crunched.png}
\end{center}
\caption{Imbalanced Graph with 196 Nodes}
\end{figure}


\subsubsection{Attributes}
A number of attributes were considered with the intention of modelling human smoking behaviour as accurately as possible, however this is a difficult notion due to the lack of detailed information relating about smoking cessation. A large part of the information that does exist, such as NHS statistics\cite{NHS-43}, is based on survey information which in turn may bring an element of bias. Generally speaking, the attributes were added on an ad-hoc basis in the early stages of development; once an idea of the kind of factors involved in smoking cessation were known through research, specific ones were implemented on the basis of simplicity and usefulness within the model.

Initially, a number of attributes were considered that extended the smoking cessation decisions into areas of lifestyle such as alcohol consumption and stress\cite{malay}\cite{NHS-44}. Early on in development, it was decided to avoid using these since the methods to simplify and represent them would remove a large part of the usefulness – for example, representing stress would require two functions, one to map produce values for stress caused externally to the system and one for stress as a result of the actions chosen. This is a complicated endeavour since modelling stress in itself is field worthy of research. A similar decision was made for alcohol consumption as attempting to model factors such as social smoking when under the influence of alcohol is difficult and again could be the focus of a modelling project itself.
The attributes chosen for the agent can be seen in table X.

\begin{figure}
\begin{center}
\label{tab:attr}
\begin{tabular}{|l||l|p{5cm}|p{5cm}|}
\hline
\bf{Name} & \bf{Type} & \bf{Lower of Range Represents} & \bf{Upper of Range Represents} \\
\hline
isSmoker 					& Boolean & False - does not smoke											& True - smokes \\
willpower					&	Double	&	0 - has no resistance to change							& 1 - has high resistance to change \\
health						&	Double	&	0 - is of very poor health e.g. has disease &	1 - is of perfect health \\
smokedPerDay			&	Integer	&	0 - no cigarettes per day										& 70 - 70 cigarettes per day \\
givingUp					&	Boolean	&	False - is not giving up smoking						& True - is giving up smoking \\
giveUpAttempts		&	Integer	&	0 - No previous attempts at giving up				& No upper limit \\
stepsSinceGiveUp	&	Integer	&	0 - No simulation steps since giving up began/is not giving up	& 500 - simulation steps since giving up began \\
sociable					&	Double	&	0 - is a social recluse											& 1	- is an extrovert \\
influenceability	&	Double	&	0 - is not easily influenced								& 1 - is easily influenced \\
\hline

\end{tabular}
\end{center}
\caption{Description of Agent Attribute Types and Ranges}
\end{figure}


The most basic attribute is that of isSmoker, which simply shows if an agent is currently a smoker or not. This is coupled with smokedPerDay, representing the number of cigarettes smoked per day, form the fundamental smoking behaviour for the model. The number smoked per day is very important since statistics indicate that heavy smokers, those smoking over 20 cigarettes a day, are less likely to want to quit than lighter smokers \cite{NHS-43} and as such, this is crucial in deciding if an agent should give up. As can be seen in the table, there is an upper cap on this value; since feedback loops are almost inevitable in the system  - and may be natural in real-world networks – this cap prevents one feedback loop causing excessive damage to the rest of the environment. In early tests without the cap, it was not uncommon for agents to be smoking upwards of 1000 cigarettes per day due to a few nodes influencing others with their higher than average smoking rates, in turn spreading this throughout the network. Although the cap is artificial, it in some way maps to a physical and reasonable limit of cigarettes smokeable in a day.

Health is another important inclusion as the quality of a person's health impacts their views of smoking and their likeliness to continue\cite{NHS-43}. Although only a basic method of including health in the model, using a value between 0 and 1 means that it can act in a similar manner as a probability. In a commercial model, this attribute could be implemented in a much more detailed way, including types of illness, how developed an illness is and a more accurate function of how health quality changes over time. There is a strong interplay between the status of the agent as a smoker and the change in health. Since the health in this model is in respect to smoking-related illness, being a smoker decreases health slightly on each turn, whereas being a non-smoker increases it.

Three separate attributes handle the process of giving up. As indicated in the research for this project, those who are in the process of giving up smoking are likely to relapse and restart smoking\cite{NHS-44} and being able to incorporate for how long someone has been giving up models the process more accurately. The isGivingUp field stores whether an agent is in the process of giving up, and can only be true when that individual is not a smoker. Furthermore, to simulate the giving up period, stepsSinceGiveUp is set to 0 whenever a giving up attempt is started and incremented on every simulation step. Obviously, a point arises where the individual no longer considers themselves to no longer be someone 'giving up' and instead just a non-smoker. The simulation does this by limiting the number of steps someone 'gives up' for, and at that point sets isGivingUp to false so that the effect of attempting to quit no longer impacts on the choice to relapse into smoking. A final aspect is that of the number of attempts at giving up someone has – again indicated by the NHS statistics, those who have attempted to give up smoking and failed before are more likely to fail again\cite{NHS-44}. By tallying the number of giving up attempts to date (incremented every time an agent changes isSmoker from false to true) this can be factored in to the decision tree.

The final attributes are those of sociability and influenceability, or how easy the person is influenced. Again values between 0 and 1, the former attempts to represent some aspect of willingness to form new social connections. Although not directly related to smoking cessation, it adds an individual trait to agents when it comes to reconfiguring their connections. The latter is used as an extra factor in deciding when agents change their behaviour, i.e. give up/begin smoking, to model the idea that in order for those who reject most influence would require a lot of sustained pressure to take on the influenced behaviour. In addition to this, it provides two extra attributes for comparison of agents, specifically in terms of assessing similarity as sociable people are more likely to interact with others who are also sociable [ref?].

In terms of assigning values to these attributes, the normal distribution was widely used for numerical attributes. Due to the abstraction of the attributes – representing largely unquantifiable concepts with decimal numbers – determining appropriate values for the means and standard deviation was a matter of balance for the model. As such, by setting the mean and standard deviation sets for all attributes as system parameters, they can be adjusted through different simulations, allowing for analysis into how this affects the network.  Generally, opting for a mean of around 0.5 and a standard deviation of 1 provides a reasonable spread of values across the agents. For the boolean attributes of isSmoker and isGivingUp, system parameters specify the probability for each being true with uniform random numbers being tested against them. Again, this allows for the user to investigate different starting states of the simulation with regard to the number of smokers/non-smokers present.

\subsubsection{Action Overview}
In regards to the `rules' section of this agent, there are three parts to each simulation step. First, the agent calculates a series of metrics in regards to its surrounding nodes, then uses these as part of a decision process organised into a decision tree. This allows the agent a chance to modify its own attributes and from there reconfigure its connections to other nodes, if necessary.
\subsubsection{Neighbourhood Actions}
In the network, every agent connected to one or more other node has a neighbourhood. This is defined to be the set of nodes that can be reached within n hops, where n is a parameter to the simulation. This represents the concept of influence through `friends-of-friends' and the assumption was made that $n=2$ was a more resonable choice since during development,  graphs of hundreds, rather than thousands, of nodes tended to have an average path length of less than 10. Having nodes draw upon more than a two-hop neighbourhood would result in a much larger portion of the network impacting a node, in terms of node count and network diameter. To avoid a case of very distant nodes playing too much of a role in influence, $n$ was kept low. In terms of attributes calculated using this neighbourhood, figure \ref{tab:neighbour} provides more information.

\begin{figure}
\begin{center}
\label{tab:neighbour}
\begin{tabular}{|l||l|p{10cm}|}
\hline
\bf{Name} & \bf{Type} & \bf{Represents}\\
\hline
infIsSmokerVal 	& Double 	&	The calculated influenced attribute of isSmoker, with $\leq 0$ being false and $\geq 0$ being true.									\\
infIsSmoker			&	Boolean	&	A boolean value representing the outcome of infIsSmokerVal							\\
infHealth				&	Double	&	The influenced attribute of health \\
infWillpower		&	Double	&	The influenced attribute of willpower										\\
infCigPerDay		&	Double	&	The influenced attribute of smokedPerDay						\\
avgCigPerDay		&	Double	&	The average of all non-zero smokedPerDay values in the neighbourhood				\\
pcSmokes				&	Double	&	The percentage of neighbours who smoke\\
pcGivingUp			&	Double	&	The percentage of neighbours who are giving up smoking (and are not just non-smokers)\\
infPcSmokes			&	Double	&	The influenced percentage of neighbours who smoke								\\
infPcGivingUp		&	Double	&	The influenced percentage of neighbours who are giving up							\\
\hline

\end{tabular}
\end{center}
\caption{Description of Agent Attribute Types and Ranges}
\end{figure}

Aside from the `ideal figure' attributes that are calculated for the current agent, a number of figures that describe the current basic state of those in the neighbourhood are calculated. Percentages of the graph which smokes and is giving up are particularly useful in creating the effect of peer-pressure within decisions and are also a possible representation of what the agent would be surrounded by in their life. This can be furthered by including the influence of the surrounding nodes into these percentages, which can be seen in figure \ref{eq:inf-perct} where $n$ is a neighbourhood node, $s$ a node of a specific dispositon (e.g. a smoker) and $n_{i}$ being the influence of $n$. This enables the system to attempt to model situations where a person's susceptibility to peer pressure depends on how influential those surrounding them are; should they be of low influence, it is unlikely that the person would adopt a behaviour that they are displaying.

FIG Y – comparison of normal and influence percentage

\begin{equation}
\label{eq:inf-perct}
\frac{\sum_{\forall s \in N} s_{i}}{\sum_{\forall n \in N} n_{i}}
\end{equation}

As described previously, the compound influence across a multi-hop route can be calculated so from this, the influence to each node within the neighbourhood can be given. Using this information, the `ideal figure' for this neighbourhood is then produced using influenced attributes by effectively averaging attributes over all of the nodes in the neighbourhood. The way in which influence attributes are calculated is slightly different for Booleans but in general, the formula in figure \ref{eq:inf-perct} is used. Booleans instead have values of true mapped to 1 and false to -1 to allow for influence to be worked into the 'ideal figure'. Once summed over the whole neighbourhood, a negative value indicates false and positive true. Notice that the sum of $\text{influence} \times \text{attribute}$ is divided by the total influence, not the number of nodes. This is because, through experimentation, division by the number of nodes resulted in very low values for influenced attributes whereas using the sum of influence gave typically more reasonable figures. Due to this, it is less a straight average, instead more of a weighed one based on the influence of a node. It should be noted that this will never exceed the values that would be generated by simply averaging out attributes since that would be a case where all nodes were at an influence of 1. Another point to note is that when calculating with smokedPerDay, cases of non-smokers (and in turn those who smoke no cigarettes) were excluded to avoid artificially lowering the average.

\label{sec:neighbourhood}
\subsubsection{Decision Tree Actions}
In order to change their own attribute values, an agent must go through a series of decisions to choose which value to change. The aim of this approach is to model a human decision making process in a logical and maintainable way, so that decisions can be rearranged, inserted or removed easily. In general, more important decisions are placed higher in the tree since they are more likely to be reached by an agent than any decision lower in the tree, by which point the decision in question becomes one in many. Due to this, when importance of decisions is discussed, those of higher importance will be higher in the tree than ones of lower importance. Throughout the development, a major focus was to uphold balance within the tree. This means that where possible, feedback loops causing extreme attribute values should be minimised and excessive clustering within the network should be avoided. Both of these had to be monitored by testing different versions of the decision tree, with changes being made as necessary.

To begin with, an approach was taken to combine a number of attributes into calculations that can be seen in figure \ref{eq:v0.9probs}, the labels referring to points in the tree as seen in figure \ref{img:init-dectree}. Generally, the tree was kept fairly shallow in favour of producing values similar to probabilities or range checks. For example, in figure \ref{eq:v0.9probs}, B shows how the decision is made based on whether the agent attributes are within 10\% of the influenced attributes – if so, the number of cigarettes smoked is adjusted. As each decision required such a combination, to build a sufficient series of decisions the same parameters would have to be formed in different ways for each choice. It became clear that this approach was flawed as it lent itself into collapsing many separate attribute choices into one. This meant that control over how each parameter effects the end choice was lost and also made it very difficult to add new decisions as most were already represented in some way in the existing steps. Furthermore, using combinations of attributes led to extra complexity being added, making it unclear exactly which attributes are causing different effects. Since a substantial part of this project is to understand what the model is showing, difficulty in isolating attributes and judging their effects is problematic. Due to this, the decision tree was heavily separated out into single attribute decisions that lead into one another.

\begin{figure}
\label{eq:v0.9probs}
	$\forall n \in N$ where $N$ is the Neighbourhood of a node:
	\begin{enumerate}
		\item p(give up smoking)=$(1-health_{n}) \times \frac{|\text{Nodes giving up smoking}|}{|\text{Nodes}|}$ \\
		\item If $n$ smokes $\pm10\%$ of the influence sum for cigarettes smoked, and $\text{health}_{n}$ is within $\pm10\%$ of the influence sum of health. \\
		\item $change = (\text{influence sum of smoked per day} - {smokedPerDay}_{n}) \times influenceability_{n}$, else there is no change. \\
		\item p(take up smoking) = $health_{n} \times \frac{|\text{Nodes giving up smoking}|}{|\text{Nodes}|} \times \frac{|\text{Nodes who smoke}|}{|\text{Nodes}|}$ \\
		\item If the influenced sum of the number of cigarettes per day is $< 0$, then $smokedPerDay = 5$, otherwise \\
		$smokedPerDay = \operatorname{round}(\frac{\sum_{\forall n \in Neighbours} smokedPerDay_{n} \times influence_{n}}{\sum_{\forall n \in N} influence_{n}})$
	\end{enumerate}
	\caption{Initial Decision Tree Probability Calculations}
\end{figure}

\begin{figure}
\label{img:init-dectree}
\begin{center}
\includegraphics[scale=0.8]{DecTreeBasic2.png}
\end{center}
\caption{Initial Decision Tree}
\end{figure}

An example of the results of the transformation that took place can be seen from equation 4 in \ref{eq:v0.9probs} to figure \ref{img:example-decision}, where each separate part of the combined decision is put into a `level' of the new tree with some stages being removed. By doing this, not only is the tree much more structured and manageable this way, but decisions can be traced through each branch, revealing how different set-ups can affect the decision making. The overall result of this stage was to move from figure \ref{img:init-dectree} to figure \ref{img:0.9-dectree}. The values on each edge represent the thresholds that need to be matched for that branch to be followed.

\begin{figure}
\label{img:example-decision}
\begin{center}
\includegraphics[scale=1]{example-decision.png}
\end{center}
\caption{Example Decision From Combination Decision}
\end{figure}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=\paperwidth,keepaspectratio]{DecTreev09.jpg}
\label{img:0.9-dectree}
\caption{Revised Decision Tree}
\end{center}
\end{figure}
\end{landscape}

Also to note is the inclusion of an irrational choice, as seen in figure \ref{img:example-decision} – this is to add a human characteristic into the simulation and introduce a small element of unpredictability. The probability of an irrational choice is a simulation parameter so can be changed easily. Importantly, the probability of an irrational choice should relate to the number of nodes in the network, since if the probability is kept the same a higher number of nodes, each making a decision every turn, will result in more irrational choices. The method of inclusion is to simply override the decision in question by logically or-ing it with a function that returns if an irrational choice is to happen. This function is a uniform random decimal number generator, only returning true when the random number is less than the probability.

All branches eventually arrive at one of two final decisions, shown in figure \ref{img:end-dec}; one decision is to either give up smoking or adjust how many the agent smokes per day, and the other is to begin smoking or adjust their willpower value. Different thresholds can be provided to these decisions in that if the threshold is exceeded, the agent will give up smoking/begin smoking. These calculations can be seen in figure \ref{eq:end-thresh}; the intention for the giving up threshold is to compare the input against the number of others in the neighbourhood who are either also giving up or not smoking, with the latter giving a higher threshold due to having similar but not quite the same behaviours. This maps to a `group mentality' approach to giving up, in that someone is more likely to give up if those around them are also doing the same. A similar concept applies for starting smoking, where the threshold instead is based on those who are smokers – should the person begin smoking, they start by smoking the influenced attribute value of the number of cigarettes. In addition to the threshold being exceeded, the agent must generate a probability less than their influenceability value, to emulate a humans potential resistance to influence. Both also have the ability for an irrational choice to override the decision.

\begin{figure}
\label{img:end-dec}
\begin{center}
\includegraphics[scale=1]{End-DecB.png}
\includegraphics[scale=1]{End-DecA.png}
\end{center}
\caption{End Decisions in the Final Decision Tree}
\end{figure}

\begin{figure}
\label{eq:end-thresh}
Given a percentage threshold $p$, a node gives up if one or more of the below are true:
\begin{itemize}
\item $\text{influenced \% giving up} > p$
\item $\text{\% non-smokers} > 120\% \times p$ and $\text{random number} < \text{influenceability}_{n}$
\item An irrational choice is made
\end{itemize}

Given a percentage threshold $p$, a node relapses if one or more of the below are true:
\begin{itemize}
\item $\text{influenced \% smoke} > p$
\item $\text{random number} < \text{influenceability}_{n}$
\item An irrational choice is made
\end{itemize}
\caption{Threshold Formulae for Decision Tree End Decisions}
\end{figure}

During the course of development, some agents were borderline between smokers and non smokers. This meant that they would switch between the two on a regular basis, potentially causing others in the neighbourhood to do the same. In an effort to this, a measure was introduced that counts the number of turns since the last change and produces a probability, seen in equation \label{eq:turns}, that slows down this rate of change by guarding the attribute swap from/to a smoker with this probability against a uniform random number.

\begin{equation}
\label{eq:turns}
p(\text{change}) = 1-\frac{1}{\text{turns since last change}\times 1000}
\end{equation}

For the giving up decision, labelled as A, either the process of giving up begins (described in the attributes section of this document) or the number of cigarettes smoked per day is adjusted. The latter is done by comparing the influenced attribute value for cigarettes smoked per day against a percentage threshold – the threshold is a system parameter and creates a range around the agent's own attribute. Should the influenced attribute be outside of this range, the agent has its attribute moved towards the influenced value by half of the influenced value. This provides a gradual change. In terms of B, a small amount of willpower is removed should they relapse to smoking, otherwise it is added. These formulae can be see in figure \ref{eq:end-attr}
\begin{figure}
\label{eq:end-attr}
End Decision A - The value of $\text{\emph{smokedPerDay}}_{n}$ is changed for some node $n$:
\begin{itemize}
\item If a irrational choice is made or $\text{\emph{influenced smokedPerDay}}_{n} < \text{\emph{smokedPerDay}}_{n}\times\text{lower \% threshold}$, then $\text{\emph{smokedPerDay}}_{n} = \text{\emph{smokedPerDay}}_{n} - \frac{\text{\emph{influenced smokedPerDay}}_{n}}{2}$ 
\item If a irrational choice is made or $\text{\emph{influenced smokedPerDay}}_{n} > \text{\emph{smokedPerDay}}_{n}\times\text{upper \% threshold}$, then $\text{\emph{smokedPerDay}}_{n} = \text{\emph{smokedPerDay}}_{n} + \frac{\text{\emph{influenced smokedPerDay}}_{n}}{2}$
\end{itemize}
Where \emph{lower \% threshold} and \emph{upper \% threshold} are system parameters.\\

End Decision B - The value of $\text{\emph{willpower}}_{n}$ is changed for some node $n$:
\begin{itemize}
\item If $n$ relapses to smoking, and a $\text{random number} < \text{\emph{influenceability}}_{n}$ then $\text{\emph{willpower}}_{n} = \text{\emph{willpower}}_{n} - \text{\emph{willpower}}_{n}\times 0.001\%$
\item If $n$ remains a non-smoker, and a $\text{random number} < \text{\emph{influenceability}}_{n}$ then $\text{\emph{willpower}}_{n} = \text{\emph{willpower}}_{n} + \text{\emph{willpower}}_{n}\times 0.001\%$
\end{itemize}
\caption{Attribute Changes for Decision Tree End Decisions}
\end{figure}

Although this simplifies the decision-making process beyond that of human cognition, it allows a basic model of peer pressure and susceptibility to general influence. There is the potential for improvement through swapping this decision series with a more accurately modelled one would be straightforward. For the purposes of this model, it provides sufficient complexity and configurability since the thresholds can be set through the decision tree as part of the tuning process.
The final decision tree had a number of extra 'decision layers' added to attempt a more accurate mapping of a real decision process; it can be seen in figures \ref{img:dectree-left} and \ref{img:dectree-right}. The aim of this was to provide a more configurable and realistic decision-making model than the previous version, whilst keeping each attribute in a separate layer. Figure \ref{tab:attr-change} shows at which point in the tree any attributes are changed (outside of the end-states A and B), which are marked as \emph{ATTR}.


\begin{figure}
\begin{center}
\label{tab:attr-change}
\begin{tabular}{|l||l|p{10cm}|}
\hline
\bf{Label} & \bf{Attribute} & \bf{Change}\\
\hline
ATTR-1 	& \emph{health} 				&	$\text{\emph{health}}_{n} = \frac{\text{\emph{smokedPerDay}}_{n}}{\text{model cigarette limit} \times 1000}$, where the cigarette limit is a system parameter for the maximum number of cigarettes an agent can smoke per day		\\
ATTR-2	&	\emph{smokedPerDay}  	&	The same change as seen for end-decision A, seen in figure \ref{eq:end-attr}				\\
ATTR-3	&	\emph{health}					&	$\text{\emph{health}}_{n} = \frac{\text{\emph{stepsSinceGiveUp}}_{n}}{\text{giving up step limit} \times 1000}$, where the giving-up step limit is a system parameter for the number of steps before someone who is giving up is classed as a non-smoker \\
\hline

\end{tabular}
\end{center}
\caption{In-Tree Attribute Changes}
\end{figure}
For the currently smoking arm of the tree, a number of changes were made. The ability to give up in the `normal' smoker range (here 5 to 15 per day, but this could be adjusted) was only allowed by an irrational choice. The rationale behind this is that whilst lighter smokers are likely to be more able to go without smoking\cite{NHS-43}, gradually reducing their own consumption until they quit completely. On the other side of this, heavy smokers are likely be stronger in their resolve against giving up smoking and as such, their willpower may important decision factor than their health due to the habit being harder to break. Health, however, is deemed more important for lighter smokers since any change in the number of cigarettes smoked is likely to have more of an impact than for a heavy smoker. Furthermore, the fact that influenced health is used aims to represent how being less healthy than those in the neighbourhood would provide more encouragement of the person to attempt to give up, with the other side of this being that if someone smokes and is of above average health, they will not feel as much social pressure with regards to said health.

For the non-smokers, the tree was expanded to include a number of new layers. If the agent hasn't failed at giving up before, then the willpower is considered most important as it represents their ability to continue the process of giving up. Health is included as a sub-point, since if someone is healthier than their neighbours they might see that as a reason to continue their giving up effort. For those who have tried to give up a few times (between 1 and 5), the percentage of non-smokers/those giving in the area dictates which route they should take which then leads into a decision based on willpower. The idea here is that since the agent has relapsed to smoking a number of times, the encouragement of the neighbourhood may have a greater effect on their own actions. In addition to this, the willpower decision is based on the influenced attribute for willpower, so the agent is effectively comparing their own resilience to those who surround them – if they are more resilient then it is assumed that they require more of their neighbours to be smoking to cause a relapse. If many (in this case more than 5) attempts at giving up have been made, a crowd mentality approach is used, where the giving up status of those around the agent dictates how susceptible they are to relapsing again. Influenced health is included since willpower appears to be irrelevant to these individuals; being more healthy than those in the neighbourhood leads to a lower chance of relapsing. Those who are not currently giving up are diverted to a more simplistic tree as they do not need their decision to be affected by how many times they have tried to quit before. Given that they have displayed the willpower to not smoke in the first place, it is assumed that this is a less important factor than their health. In terms of health, the agent's own value is compared to the influenced attribute in an effort to model an attitude of health-awareness; if someone is of good health relative to others then they are less likely to risk it by smoking, whereas if they are of below average health, they may perceive smoking to make little difference.

The end-state decisions for each branch have a percentage threshold stated – the values shown on the diagram have been selected through a process of trial and error with the aim of providing as balanced a simulation as possible. Since some decision routes are more popular than others, the percentages need to account for this. With this in mind, they also represent the relative effect of that branch on the chance of giving up. An example is that if someone is a light smoker (< 10 cigarettes a day), has never tried to give up before and has higher than the influenced health, then it would be reasonable for them to be hard to convince to give up as they don't smoke a lot and are, compared to their social connections, quite healthy. To show this, that particular end-state requires more than 70\% of their neighbours to also be giving up before they will do the same.

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=\paperwidth,keepaspectratio]{Dectree-left.png}
\label{img:dectree-left}
\caption{Smoking Arm of the Final Decision Tree}
\end{center}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=\paperwidth,keepaspectratio]{dectree-right.png}
\label{img:dectree-right}
\caption{Non-Smoking Arm of the Final Decision Tree}
\end{center}
\end{figure}
\end{landscape}

In general, whilst this configuration of the decision tree was settled upon for performing simulations using the ABM, extra levels could easily be added or decisions rearranged. Ideally, this kind of decision-making process would be developed alongside in-depth psychological studies into the field, so that an accurate and expansive tree can be built. Because this was beyond the scope of the project, a more ad-hoc approach was used by designing the tree in accordance to model balance and real-life comparison, leading to some assumptions about human behaviour having to be made.

\subsubsection{Connection Reconfiguration Actions}

The final stage of the agent actions is to reconfigure their connections to the neighbourhood. By this stage, the agent has assessed the neighbourhood and made changes to their attributes where appropriate so based on this new information, the agent can now look for any relationships to others who are now too dissimilar or for new matches that are closer to their interests. From the initial research and development stage of the project, the chosen method of comparing agents was to generate a scoring algorithm, seen in figure \ref{code:scoring}. This way, any two agents could be compared at any point and a figure to represent their similarity produced.

Different attributes will have different weightings when it comes to comparing agents. As the algorithm is intended to imitate the process of a human looking to their circle of connections for other similar people, someone's willpower is likely to have less impact than whether they have the same smoking behaviours. To account for this, the scoring algorithm uses different score values for each attribute – this can be seen in figure \ref{tab:reconfig}. Another factor in the weighting is that of how the difference is scored. The algorithm offers three options, the formulas for which can be seen in \ref{code:comp-methods}: percentage difference, attribute comparison and linear scoring.

Percentage difference is most commonly used since it provides a scaled score within one interval plus and minus of the current agent's attribute score. Obviously this means slightly different things for different attributes but in general, the higher scoring someone is in an attribute, the more accepting they are of others; an example might be that light smokers would consider someone who smokes a few more cigarettes a day more to be of similar difference to themselves as a heavy smoker would to someone who smokes half their intake. Attribute comparison is a more straightforward mechanism, simply handing out an amount of points should two attributes be the same, or in the case of stepsSinceGiveUp, below a certain value. Finally, linear scoring is only used by influenceability and is a way to incorporate the susceptibility of someone to peer pressure into the score. No scoring is done against the other agent for linear scoring, since it is based on the attributes of the current agent only. A disadvantage of this method is that it greatly simplifies what humans might look for in social ties as it assumes that they are only interested in those who behave similarly. Obviously this is not the case in reality though for the sake of avoiding modelling this connection seeking in a high level of detail, the assumption will stand for the project. If required, the scoring method could be substituted for one which accounts for one which handles the interplay of personality in a more detailed manner.

\begin{figure}
\begin{center}
\label{tab:reconfig}
\begin{tabular}{|l||l|l|l|}
\hline
\bf{Attribute} & \bf{Minimum Score} & \bf{Maximum Score}\\
\hline
isSmoker 						& 0	&	1\\
givingUp						&	0	&	2\\
health							&	0	& 5\\
willpower						&	0	&	5\\
sociable						&	0	&	2\\
influenceability		&	0	&	3\\
stepsSinceGiveUp		&	0	&	5\\
smokedPerDay				&	0	&	5\\

\hline

\end{tabular}
\end{center}
\caption{Scoring Algorithm Upper and Lower Bounds for Scores}
\end{figure}

\begin{figure}
\begin{center}
	\begin{lstlisting}
for(Node n : Neighbours)
{
	//Attribute comparison
	if(current.isSmoker == n.isSmoker)
	{
		score + 1
		//Attribute comparison
		if(current.isGivingUp && n.isGivingUp)
		{
			score + 2
			//Attribute comparison score
			if(n.stepsSinceGiveUp less than 1.5 times current.stepsSinceGiveUp)
				score + 5
			else
				score + 2
		}
		//Attribute comparison
		if(current.isSmoker)
		{
			score + percentScore(smokedPerDay) 
		}
	}
	score + percentScore(health)
	score + percentScore(willpower)
	score + percentScore(sociable)

	score + linearScore(influenceability)

	return score/28
}
	\end{lstlisting}
\end{center}
\label{code:scoring}
\caption{Scoring Algorithm Pseudocode}
\end{figure}

\begin{figure}
\label{code:comp-methods}
\begin{itemize}
\item \emph{Linear Scoring} - The attribute is multiplied by the maximum s
\item \emph{Percentage Scoring} - The percentage difference between attributes is calculated ($\frac{\text{attr}_{this}-\text{attr}_{other}}{\text{attr}_{other}}$). If they are separated by more than 100\%, the minimum score is given, otherwise $\text{\emph{score}} = \text{\emph{maximum score}} - \text{\emph{\% difference}} \times \text{\emph{maximum score}}$.
\item \emph{Attribute Comparison} - Two values are compared and if they are the same (or within some defined range), the maximum score is given, otherwise a lesser score is given.
\end{itemize}
\caption{Comparison Methods Between Agents}
\end{figure}

Once a score has been calculated and normalised to between 0 and 1, this can be used to determine if the two agents should form a connection, remove an existing one or do nothing. The method for this is to have two simulation parameters, ConnectionScoreRemoveBound and ConnectionScoreAddBound, which dictate the boundaries for action to be taken. The `remove boundary’ must be lower than the `add boundary’ and if the score is below this value, the edge is remove. Should the agent score be above the add boundary, then the edge is added. New edges have their influence set through a normal distribution with a mean of their similarity score, with 0.5 being deducted to avoid the mean exceeding 1 and preventing the value from one above the boundary (meaning if the score was set to be the similarity score as a mean, all edges would probably be of high influence since to be connected, they must be of high score) to a more central position. Experimentation has shown this to give a more reasonable distribution of influences.

It should be noted that as part of the effort to keep the model balanced, limits were imposed on the number of connections that each node can have. Should another edge be added when the node has reached its limit, the lowest influence edge is removed in favour of the new connection. Once again, the limit a simulation parameter so can be changed to investigate the effects that is has, though testing indicated that it helped avoid clustering and excessive connection-forming with other nodes. Especially important in this area is the boundary values. By lowering the upper bound to 0.6, the graph significantly increased its chance to become very dense and turn into a tight cluster  (figure \ref{img:ex-crunch}), whereas a raising it to 0.7 maintained a much better balance and avoided the clustering, as seen in figure \ref{img:ex-noncrunch}.

In order to introduce external influences to a neighbourhood, every agent has a small chance to form a random connection to some other agent in the graph. This is deliberately set to have a very low occurrence rate as testing showed that setting the probability to be any more than 0.002\% of sociability lead to regular addition of edges, effectively rendering the scoring algorithm useless and resulting in an incredibly dense graph. By making this a rare event, it attempts to model the day-to-day chance of a new influence entering a human's life i.e. that of meeting a new person or taking on a new role-model. Over the course of the simulation, there is a chance that nodes may become disconnected from others. As it is slightly unrealistic for humans to have absolutely no social connections, the model reconnects them to a random node in the network with a 30\% chance of connection. This is not a certain reconnection since to reach a disconnected state, the edge must have held attributes such that it fell below the removal boundary of its neighbour nodes.

% --------------------------------
%
%		END OF IMPLEMENTATION
%
% --------------------------------

% --------------------------------
%
%			  RESULTS
%
% --------------------------------
\chapter{Simulation Results \& Model Analysis}
\section{Overview}

To understand whether the developed model provides useful results and meets the initial specification, two separate methods were used. Firstly, using the model to run simulations provides a number of pieces of information. In regards to the problem domain, finding out new information about the impact of social networking upon smoking is a key aim, but at the same time, these simulations should help to demonstrate whether the model is balanced, efficient and a good base for future work. Following this, analysis of the model from a qualitative viewpoint will form the second part of this section. As a part of the specification concerns itself with understanding whether the model is a suitable proof of concept for a commercial model, this is combined with general thoughts on the quality of the model and any future improvements. The basis of this analysis comes from both the design decisions and the results of the simulations carried out.

\section{Simulation Analysis}
Using the developed model, a wide range of possible situations can be simulated through using generated or sampled graphs along with configuring certain percentages of the population to smoke, give up or not smoke at all. Whilst running all of these simulations is beyond the scope of this project, ones from prominent categories were run instead. These categories were simulation parameters, large networks, sampled networks, and composite networks.

For all simulations, 30,000 simulation steps were carried out with graph and attribute files being exported every 1000 steps. A simulation step is a difficult concept to assign a unit of time to since it is discrete and involves making decisions, assessing influence and rearranging social ties all in one movement. In light of this, the simulation length, whilst not given a specific time period, should be considered to represent months and years rather than hours and days.

\subsection{Simulation Parameters \& Agent Attributes}






\subsection{Decision Tree Analysis}
Decision Tree Analysis
As the decision tree is the major factor in selecting the behaviours of agents, the number of 'hits' upon each decision reveals a lot about how individuals are guided to their final choice. To get a dataset for this, a scale-free graph with 483 nodes, 20\% smokers and 10\% giving up in order to be close to the UK smoking statistics [REF], was simulated over a period of 30000 steps with every decision made by an agent being recorded. The final percentages of smokers was around 9\%, a moderate decrease in quantity but by no means significant due to the ratio of smokers to non-smokers. Figures \ref{img:dectree-right-lab} and \ref{img:dectree-left-lab}  shows the decision tree labelled with both the percentage of total 1hits' and the percentage share of hits for the parent decision.

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=\paperwidth,keepaspectratio]{dectree-right-labelled.png}
\label{img:dectree-right-lab}
\caption{Non-Smoking Arm of the Final Decision Tree. Labelled to indicate flow of decisions, where O is the percentage of all decisions and P is the percentage of the decision from its parent.}
\end{center}
\end{figure}
\end{landscape}

\begin{landscape}
\begin{figure}
\begin{center}
\includegraphics[width=\paperwidth,keepaspectratio]{dectree-left-label.png}
\label{img:dectree-left-lab}
\caption{Smoking Arm of the Final Decision Tree. Labelled to indicate flow of decisions, where O is the percentage of all decisions and P is the percentage of the decision from its parent.}
\end{center}
\end{figure}
\end{landscape}

In regards to overall percentage share of hits, the values appear as expected; the `non-smoking' arm of the tree encounters more hits than the smoking arm since a greater portion of the network does not smoke than does. Within the smoking arm, 70\% of agents fall in the middle branch, i.e. the 'moderate' smoker, meaning that they do not have a high chance of giving up in that decision route. Furthermore, 20\% of agent hits were to the 'heavy' category, where although they do have a direct chance of quitting it is unlikely due to the attributes used in that subtree. From this it may be deduced that to get a greater proportion of these smokers to have a chance to give up, they would need to be surrounded by people who smoke less than they do so that their influenced attribute for the quantity of cigarettes smoked would decrease. In turn, this would let them fall into the 'light' smoker category which has a higher chance of giving up. Another explanation for the underrepresented section of light smokers is that few hit this decision regularly because instead, they give up smoking. Interestingly, if the end-states of the light smoker sub-tree is looked at, both choices left a greater proportion of the agents going to the option which required a higher local percentage of non-smokers to give up, which implies that there is a core of light smokers who do not give up. To have the previously described scenario of lighter smokers encouraging heavier ones to reduce consumption, a greater percentage of lighter smokers would be required over the course of the simulation whilst avoiding skewing the tree such that it is not made too difficult to give up.

Of the medium and heavy smoker groups, looking at deeper levels of the tree points to which areas could be targeted to encourage cessation. For the former group, the majority of hits resulted in the agent reassessing their smoking consumption, with the majority of this sub-group opting to reduce consumption. Some, however, were given the chance to quit with the larger portion taking the marginally easier route of 70\% of the neighbourhood needing to be not smoking. This matches with the above idea of getting smokers down to lower consumptions before giving up, which could again be improved by planting more, high influence, low consumption smokers in the area. For the 'heavy' group, there is a fairly even split across all sub-groups. It should be noted that there is an 'easy' chance for heavier smokers to give up if their health is lower than those they are related to, though this was not any more popular than any of the other decision routes. This indicates that to get this group to stand more chance of quitting, they would need to be surrounded by people who smoke either less than they do or not at all, and also are of better health. Doing this would guide them to either the light smoker options or the 'easy' option within the heavy smoker tree.

For non-smokers, it is most important to direct them away from end-states where there is a chance to either begin smoking or relapse whilst giving up. For group of agents giving up, the most common branch to follow was for those who have tried to give up between 1 and 5 times. As a side note, this is encouraging for the model balance as it shows that the majority of individuals do not fall into a cycle of giving up and relapsing, causing a very high number of give-up attempts. Within this group, the vast majority had more than 30\% of their neighbours as non-smokers so had a lower chance of relapsing. Due to the fact that willpower was the next factor to decide upon, there was a mostly even split. This is the key part of the branch since it shows that by having a contingent of not only non-smokers, but also agents of lower willpower with high influence will lead to the individual finding it easier to avoid smoking again. The section of agents with more than 5 give up attempts were most likely to be surrounded by fewer others giving up – intended to emulate the effect of group support to those who have failed multiple times before - meaning that they had a higher chance of relapsing. In the simulation it can be seen that all of the agents had lower than the influenced attribute for health, so stood a much higher relapse chance. To avoid this situation, individuals who have failed multiple times before need to be surrounded by a high quantity of other giving up, ideally of similar health to their own to reduce their chance of relapse. The final section of the `giving up' sub-tree is that of the people with only one giving up attempt. Although most agents were of willpower higher than the threshold, the split in health reveals that to increase their chances of not beginning smoking, others of similar or lower health need to be in their neighbourhood. This also holds true for those of lower willpower.

The final section of the tree is that of non-smokers who are not currently giving up. As implied by the final smoker/non-smoker percentage, the majority of the decision hits of the graph were in this area. On the next layer of the tree, a slight skew towards the agents being of higher than their influenced health and to maintain a higher chance of avoiding smoking being taken up is to ensure that the agent is surrounded by people of similar health. At the next decision down, willpower is used; a higher willpower here results in a more desirable chance of not relapsing. Since this is not affected by the willpower of those in the neighbourhood, the most important decision here is that of the health. To be more specific, those of low willpower should be in a neighbourhood of lower health but high influence agents, whereas higher willpower individuals can afford to be around those of similar or higher health.

In general, the decision tree reveals a number of pointers for how to drive the network in the direction of giving up smoking then avoiding relapsing. Having smokers be convinced to either smoke less per day or be surrounded by healthy, non-smokers helps in getting them to quit whilst positioning those who are giving up in the correct neighbourhood has a large effect on their chance of relapse, the exact composition of this neighbourhood depending on how many times the person has tried to quit before.

\subsection{Sampled Networks}










\section{Model Analysis}
\subsection{Commercial Analysis}

Commercial viability analysis of this project mainly focuses on whether the current system could be used as a starting point in building a more in-depth model so looks at aspects such as scalability, efficiency and extensibility. This analysis assumes that a commercial-level model should be:
\begin{itemize}
\item \emph{Efficient} - to save excess computing cycles on large and long-running simulations
\item \emph{Scalable} - to be able to run both small and very large models, allowing for analysis on a wide range of social situations such as small friendship groups to populations of cities or towns.
\item \emph{Extensible \& adaptable} - should new work influence the configuration of the model, it should not need entirely rebuilding in order to include these changes.
\end{itemize}

The efficiency of the model in places is good – the decision tree, for example, primarily uses if statements for decisions with only basic arithmetic comparisons or pre-computed values rather than more complex values computer for each stage. Furthermore, the neighbourhood and influenced attributes are only calculated once, since these are both intensive operations and in a similar manner, the agent tries to calculate datasets such as this once per turn at most.  Due to this system being a proof-of-concept, efficiency was not always the biggest concern during development with the favour instead being given to a balanced model. This means that some inefficient sections are present, but are not so intertwined with other components in such a way that makes them difficult to fix.   

The first example is due to the fact that the network structure can change from one agent's turn to another, neighbourhoods have to be generated for every agent every turn which is not very efficient. This requires a lot of graph traversal and in larger graphs, a lot of processing time. Unfortunately, this is difficult as caching neighbourhoods introduces the chance of nodes who are newly removed from this set playing a role in calculations and verifying the neighbourhood requires the same amount of computation as creating it. One possible way to get around this is to cache the neighbourhood at the end of each agent's turn. Changes could then be handled by, again at the end of each turn, having each agent broadcast its one-hop neighbours to its one-hop neighbourhood. These nodes can then, between the end of their turn and the start of the next, check for the existence of neighbourhood nodes – new ones can be added, whilst if they are unaccounted for then they can be removed.

A good level of efficiency in the agents lends itself to scalability of the model. This is because the majority of the runtime is spent within agent actions so if these are made to be as efficiently as possible, then the model will perform better when more agents are added. Because this model is not developed with the intention of running simulations in excess of 10,000 nodes, little emphasis has been given to developing with the intention of running large scale tests though in a commercial solution, this would be very important. Developing in this manner requires attention to be split between individual agents and the system as a whole; for example, removing any actions carried out by every agent that could be done once for the whole network would save a lot of compute cycles. If performance was of paramount importance, then the stage at which network reconfiguration happens could be separated out from within the agent to for the whole network at once. This means that rather than agents would work on a fixed network each turn rather than one which changes between separate agent turns meaning steps such as scoring between agents would need to be carried out fewer times.

With regards to scalability, some changes would need to be made in order to provide good performance on large systems. Agent-based systems are difficult to scale due to the amount of processing power required for even smaller ones as even in this project, a basic representation of humans and their connections performed many different operations per step. As more agents are added, on top of the internal processing for this new addition, each existing agent may now have even more nodes in its neighbourhood, resulting in more calculations per agent. The exact effect of more agents being added can depend on a number of factors, such as its degree, size of neighbourhood and more, making it difficult to quantify.

To better analyse this, simulations and graph generations were carried out on a variety of graph sizes on both generated small-world and scale-free graphs. Each simulation ran for 30,000 steps and was timed, with the results being plotted. For reference, the tests were carried out on a 3.4GHz quad-core, Intel i5 processor, using 1.3GB of RAM for the Java Virtual Machine. Graph generation is a one-off step in a simulation that, in its most complete sense, will create a graph of size n agents each with a set of attributes and social connections. It should be noted that if a sampled graph is used, the time taken for a n nodes to be prepared for running will be less than that of a scale-free graph being generated since the sampled graph simply uses the attribute creation step that scale-free graphs also use.

\begin{figure}
\label{img:SF-gen}
\begin{center}
\includegraphics[scale=0.4]{scale-free-gen.png}
\end{center}
\caption{Scale-Free Generation: Time taken (ms) against the size of graph generated where a lower time is more desirable. Note the almost-quadratic increase as the number of nodes increases.}
\end{figure}
\begin{figure}
\label{img:SW-gen}
\begin{center}
\includegraphics[scale=0.4]{small-world-gen.png}
\end{center}
\caption{Small-World Generation: Time taken (ms) against the size of graph generated where a lower time is more desirable. A linear trend is shown as the number of nodes increases, whilst the overall time to generate is very low.}
\end{figure}

As visible in figure \ref{img:SF-gen}, the generation time for scale-free graphs increases in an almost quadratic fashion as the number of nodes increases, reaching around 30 minutes creation time for a 3000 node graph. Adding to this, as the generation method leaves disconnected nodes in the graph, many of these have to be removed causing the operational node count to be often lower. Obviously, this level of performance would not be effective in a commercial environment since large graphs of over 1000 nodes would take too long to create using this method. This, however, may be partly due to the implementation rather than the method; figure \ref{img:SW-gen} shows the generation times for a small-world graph, using the Repast built-in small-world creation algorithm. A very short creation time across the range of node counts tested with a linear increase can be seen, which is a better performance however the method for generating small-world graphs is much less intensive. At least some of the cause of this is the fact that the calculation used to determine if an edge is to be added or removed is simpler for small-world graphs than scale-free. From this, it can be seen that to make this aspect of the model more commercially viable, it needs to offer a more scalable generation method for scale-free graphs.


\begin{figure}
\label{img:SF-run}
\begin{center}
\includegraphics[scale=0.4]{scale-free-runs.png}
\end{center}
\caption{Scale-Free Runs: Time taken (ms) against the size of graph generated where a lower time is more desirable. Although steep, as the number of nodes increase, the time taken for that run increases in a linear fashion.}
\end{figure}
\begin{figure}
\label{img:SW-run}
\begin{center}
\includegraphics[scale=0.4]{small-world-runs.png}
\end{center}
\caption{Small-World Runs: Time taken (ms) against the size of graph generated where a lower time is more desirable. As with small-world generation (figure \ref{img:SW-gen}), the time taken increases linearly with the number of nodes in the run.}
\end{figure}

Perhaps more important than this is the time taken for simulations to run. This is a difficult area to quantify since two graphs of identical node count can vary in many ways such as edge density, average degree and more, in turn affecting how the graph may behave. Using the graphs generated for the previous tests, the results of the simulations carried out can be seen in figures \ref{SF-run} and \ref{SW-run}. A point to note is that due to the scale-free method not producing graphs of exactly the number of nodes required, creating a 3000 node graph to match the upper limit of the small-world tests would have been very difficult. It can be seen that both follow a linear increase as nodes are added, with simulations of around 750 nodes taking around 4 minutes for a small-world graph and just over 5 minutes for a scale-free one. The similarity in the trends of these graphs indicate that the type of graph may be independent of how long a simulation on it takes. If this is the case for all graphs, then the focus for scalability improvements lie within agent efficiency, as detailed above. In regards to the simulation times at the moment, whilst 20 minutes for a 3000 node small-world graph may seem high, if run on a cluster or HPC, the runtime would be much shorter. As such, increasing the number of agents significantly would not cause untenable runtimes, should the requisite computing power be available. From this basic investigation, the model appears to be fairly scalable but would benefit from work on agent efficiency if simulations were to take place on personal computers.

On the whole, the system displays a good level of extensibility and ease of maintenance. From the start of development, focus has been on building not only a useful model but also a platform on which makes it easier to improve said model. This was done by ensuring that any tools developed were done so in as general a manner as possible, as well as grouping them into related packages. The agent itself separates out its actions into methods, especially the three steps of neighbourhood finding, running the decision tree and reconfiguring connections. A useful feature that this allows is the easy replacement of any of these components, without having to disentangle them from the rest of the agent implementation – a company could use this to create a number of separate approaches to the decision tree and network reconfiguring and test them by simply changing the method calls.

Furthermore, the Repast framework is a large, public project which causes it to be well organised and ensures that user code is separated from system code. By making user code interface with the system instead of building within it, modifications to said system can be avoided and the efficiency and speed of the underlying simulator can be guaranteed. This is important since it shifts the emphasis for code to be well written and efficient to the model and relating tools thus. Furthermore, keeping this separation means that if the model implementation is maintainable, the developer does not need to worry about the simulation environment code. Although this is very useful, the model is developed within Repast Simphony whereas a commercial level model would probably require HPC access and due to this would need to be rewritten in C++. Assuming the same principles would be used in that model as in this, this task would not need as much time or effort to develop and so would not be a major problem.

As a final note, whilst the original intention was to work with Sandtable, an agent-based modelling company, commercial pressures during the course of the project meant that they had less involvement that planned. Although this was the case, a high-level review of both the social networking and decision making methods was undertaken, with feedback being positive. Even though this was the case, the project success was not affected since a more general view of commercial needs was adopted.

\subsection{Model Analysis}
Overall, the model did provide some insight into the problem domain and from this is in some respects successful. Considering the fact that the intention was for the system to hold a proof-of-concept status, keeping the modelling of both the agent and network to basic levels, it is encouraging to see that the project does seem to exhibit behaviours that would be expected such as being able to run on networks without them collapsing into tight clusters but at the same time with the network actually changing. Furthermore, as described above, it does produce results that indicate how networks can be manipulated in order to aid in giving up smoking. With this in mind, a number of other points have been discovered that are important in the development of this type of model.

A major factor during the development of the project was that of balancing the model. Trial and error indicated that balance was indicated by how long the social network could maintain a structure similar to its beginning state; for example, if a scale-free network collapses into a small-world network in fewer than 10,000 simulation steps, then the model would appear to be very imbalanced, whilst a collapse over a longer period of time would indicate a better configuration. It was found that certain aspects of the system had a much greater impact on this balance than others:
\begin{itemize}
\item \emph{Social connection add/remove boundaries} - a lower bound of 0.4 and upper bound of 0.7 was used throughout the simulations though whilst finding this boundary set, many others were tried. As might be expected, raising the lower bound too high caused the graph to become sparse over time, since it would be overzealous with edge removal. On the other hand, dropping the upper bound by just 0.05, the network became saturated with edges very quickly, resulting in many feedback loops and lots of edges becoming high influence.
\item \emph{Decision tree end-state inputs} - the values which are fed into the decisions that choose if a person begins or stops smoking significantly affects the distribution of smokers to non-smokers. Setting the thresholds too low makes it very easy for that decision to cause a change such as smoker to non-smoker, meaning that agents will readily make this change and flick between two states regularly. In contrast, setting the values too high means that extraordinary situations are required for changes in behaviour, which may not happen. This would cause very little change in the graph and could cause it to become stale, reaching a saddle-point very quickly.
\item \emph{Decision ordering} - the order in which an agent reaches decisions changes the significance of attributes since on average, the higher in the decision tree it is the more agents will encounter it. For example, the most important decision is for whether the person is a smoker in the tree in this model, so it is at the top. If, for example, willpower was placed at the top of the tree, then any changes to willpower would have a much greater effect on agent behaviour.
\item \emph{Influenced attributes} - given that these are generated based on the autonomous behaviour of the agent to the point of calculation, there is a chance that influence causes these values to tend towards the extremities of their ranges. Should these attributes be used as the basis of decisions within the tree, this can cause entire branches to be inaccessible to agents, heavily skewing the balance of the tree and by extension, the model.
\end{itemize}
If the model were to be used for further research, these areas should be the focus when it comes to rebalancing, if necessary.

Behaviour-wise, the model performs generally as expected but with some areas for improvement. The fact that it can be configured to avoid mass clustering indicates that the interaction between agents performs in a similar but basic manner compared to human behaviour. This appears to be done by maintaining a number connections that are of different influence level with others whilst avoiding having a very high number of these connections – much like a person would find it difficult to maintain a lot of friendships simultaneously. Furthermore, the manner in which connections are reconfigured takes uses abstract representation of evaluating likenesses, meaning that similar people aim to connect with each other and, due to this connection, then influence each other. In social networking terminology, this is homophily, as described in the Literature Review section of the document. Given that this is observed in real-world networks, its occurrence here is useful.

Areas in which some more unexpected behaviours occurred were, on the whole fairly specific apart from the fact that attaining consistent results is difficult. Over a number of runs, the same simulation configuration can give a wide range of finishing splits between smokers and non-smokers which means that for results to be determined, many iterations must be run then averaged. Even if this is the case, should the spread of results be uniform across the range, this would imply that the particular parameters used have no effect on the final state. This is hard to identify because the model relies on agent autonomy and in doing so, a certain variance between iterations should be expected; as such, a lack of consistency may be endemic due to the choice of approach. Tackling this involves paying attention not only to the number of iterations used but also the variance and distribution of the results.

Some of the more specific undesirable behaviours, whilst small, have an impact on the model at large. Firstly, it seems that the model makes beginning smoking (particularly relapsing) too easy resulting in a number of give-up attempts over the course of the simulation or, in extreme cases, constant switching between being a smoker and non-smoker. This is not ideal as the spurious changes drive up the number of attempts and giving up, effectively nullifying the impact of previous attempts, as well as them not providing a source of influence consistent with one behaviour. Adding to this, by the end of the simulations there was very rarely any individuals giving up. As agents that do give up and then relapse increase their chances of doing so in the future so if this is repeated a number of times, the length of the runs (30,000 steps) will mean that any attempts to give up are likely to fail rapidly. Ultimately this is a model stability issue and to address it, a better method of having willpower and previous attempts at giving up would need to be developed that could incorporate the time since the last effort and possibly other attributes. Doing this would also contribute towards solving the issue of it being too easy to begin smoking, especially those who are at risk of relapsing.

From the results above, it is clear that the decision tree for cessation and social networking part of the model do have worth since they provide the opportunity to manipulate them in a number of ways or to track their behaviour, allowing for analysis of particular model configurations. The decision tree provides a good way to keep the decision-making process simple whilst offering a range of different choices to the agents and because of how the values for said decisions can be changed, the balance of the tree can be adjusted with ease. The network itself proved to be a good representation of real-life networks, allowing the opportunity for both generated and sampled networks to be simulated upon. Influence within the model was built into the decision tree ensuring that the influence did actually affect the choice of behaviours. This was further reinforced by the end-states in the tree relying on the behaviour of the neighbourhood; in retrospect, this weighs a little too heavy in favour of the actions of those around the agent so the inclusion of willpower or stubbornness would help to recreate those who will behave in a certain way regardless of their surroundings. Obviously, for a commercial level model, the tree would be greatly expanded with more attributes added to the agent; this would then give further configurability and more insight into the different areas of smoking cessation. Coupled with further integration of influenced attributes into the tree, the decision tree and graph approach is promising.

\subsection{Further Improvements}
Given that this model is a starting point rather than a complete solution to the problem, there are many possible changes that would bolster the performance and accuracy. These range from minor changes to reworking of entire sections and in general were not included due to time constraints or simply being too complex to include.

The agent itself could be adjusted to include more attributes and for those already included, more accurate models. The former is a difficult proposition since to make these additions, it should be the case that the attributes have been shown to have some effect on smoking through other research. Due to how circumstantial smoking behaviours seem to be, acquiring general information about factors in smoking cessation is not straightforward. As such, to undertake this, the best route would be to undertake specialist research in the field with the specific intention of understanding smoking behaviour contributing factors. In spite of this, some attributes such as stress could be included, assuming a reasonable model could be incorporated which would account for both internal and external contributors to the stress. Accurately modelling existing parameters is easier since as shown in the Research \& Literature Review section (\ref{sec:litrev}), work towards this has already been done. The process for each attribute would involve isolating each concept and investigating it individually, through which a mathematical model could be developed for it. To prove the accuracy, these models would then have to be tested against some real-world measure. Whilst this would aid in the general result quality of the model, it could incur extra computational cost which may be a problem if large simulations are the main focus. Avoiding this would then require either efficient attribute representations or only modelling the most significant ones in such a detailed manner.

In terms of the decision tree, there are a number of design-level changes that, whilst would not impact the operation of the tree, would allow for much faster and easier tree prototyping, in turn making it more straightforward when developing new or adapting existing trees. A modular approach could be used where a decision is represented by some abstract type, allowing the specification of which attributes to decide on and their relevant thresholds for each as well as some list of which decisions lead to others. This would remove the need for extra programming when implementing new decision trees so would make the model more usable for those who do not know how to program. On top of this, it would be easier to build visualisation tools for and get an understanding of the tree at a glance compared to the current implementation which uses nested if statements.

Another decision-tree based change would be the inclusion of `dead-zones', which are tree states that result in no change to behaviours and/or attributes. By defining more `no change' behaviours, this would provide another aspect of configurability and also, by offering the ability to not change, produce more gradual results. An extension to this would be to separate out physical and mental attributes. This would allow for influence, for example, to act upon mental state which in turn acts upon the individual's decisions and behaviours. Combined with the idea of dead-zones, gradual changes could be made to the mental attributes resulting in more of a `decision-making over time' behaviour by the agent.

Finally, some extra tools would be very useful in order to produce graphs more accurate to real-world ones. Examples of this would be to generate graphs that could have configurable clusters such as groups of smokers, or of highly connected nodes. This would provide better emulation of smaller, dense social circles within a larger network, similar to groups of friends and their interactions with other groups. This would require elements of both small-world and scale-free graphs, so neither method alone would be suitable to generate them. Additionally, some method of specifying specific attribute changes within agents at certain points of the simulation would be very useful in investigating how changes within a subset of nodes affects the network at large. This would allow a network to be set up and run for an amount of time before some pre-determined changes happen, providing a more organic simulation compared to starting with the network changes required.


% --------------------------------
%
%			END OF RESULTS
%
% --------------------------------

% --------------------------------
%
%			  CONCLUSION
%
% --------------------------------
\chapter{Conclusion}
% --------------------------------
%
%			END OF CONCLUSION
%
% --------------------------------
\bibliographystyle{ieeetr}
\bibliography{biblio}
\end{document}